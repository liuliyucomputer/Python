# unicodedataæ¨¡å— - Unicodeå­—ç¬¦æ•°æ®åº“
# åŠŸèƒ½ä½œç”¨ï¼šæä¾›Unicodeå­—ç¬¦çš„æ•°æ®åº“è®¿é—®åŠŸèƒ½ï¼Œæ”¯æŒå­—ç¬¦å±æ€§æŸ¥è¯¢å’Œåç§°è½¬æ¢
# ä½¿ç”¨æƒ…æ™¯ï¼šæ–‡æœ¬å›½é™…åŒ–å¤„ç†ã€å­—ç¬¦è§„èŒƒåŒ–ã€å­—ç¬¦å±æ€§æŸ¥è¯¢ã€ç‰¹æ®Šå­—ç¬¦å¤„ç†
# æ³¨æ„äº‹é¡¹ï¼šå¤„ç†å¤šè¯­è¨€æ–‡æœ¬æ—¶éœ€è¦æ³¨æ„Unicodeè§„èŒƒåŒ–å½¢å¼çš„é€‰æ‹©

import unicodedata
import re

# æ¨¡å—æ¦‚è¿°
"""
unicodedataæ¨¡å—æä¾›äº†å¯¹Unicodeå­—ç¬¦æ•°æ®åº“(UCD)çš„è®¿é—®ï¼ŒUCDåŒ…å«äº†å…³äºUnicodeå­—ç¬¦çš„å„ç§å±æ€§ä¿¡æ¯ã€‚
ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
- è·å–å­—ç¬¦çš„åç§°ã€ç±»åˆ«ã€æ•°å€¼ç­‰å±æ€§
- æ‰§è¡ŒUnicodeè§„èŒƒåŒ–ï¼ˆNFCã€NFDã€NFKCã€NFKDï¼‰
- æŸ¥è¯¢å­—ç¬¦çš„åˆ†è§£å’Œç»„åˆä¿¡æ¯
- è·å–å­—ç¬¦çš„åŒå‘ç±»åˆ«ã€é•œåƒå­—ç¬¦ç­‰ä¿¡æ¯

unicodedataæ¨¡å—åœ¨å¤„ç†å›½é™…åŒ–æ–‡æœ¬ã€å¤šè¯­è¨€æ”¯æŒã€æ–‡æœ¬è§„èŒƒåŒ–ç­‰åœºæ™¯ä¸­éå¸¸æœ‰ç”¨ã€‚
"""

# 1. å­—ç¬¦å±æ€§æŸ¥è¯¢
print("=== å­—ç¬¦å±æ€§æŸ¥è¯¢ ===")

# è·å–å­—ç¬¦åç§°
print(f"å­—ç¬¦'A'çš„åç§°: {unicodedata.name('A')}")
print(f"å­—ç¬¦'Ï€'çš„åç§°: {unicodedata.name('Ï€')}")
print(f"å­—ç¬¦'ä½ 'çš„åç§°: {unicodedata.name('ä½ ')}")
print()

# è·å–å­—ç¬¦ç±»åˆ«
print(f"å­—ç¬¦'A'çš„ç±»åˆ«: {unicodedata.category('A')}")  # Luè¡¨ç¤ºå¤§å†™å­—æ¯
print(f"å­—ç¬¦'a'çš„ç±»åˆ«: {unicodedata.category('a')}")  # Llè¡¨ç¤ºå°å†™å­—æ¯
print(f"å­—ç¬¦'1'çš„ç±»åˆ«: {unicodedata.category('1')}")  # Ndè¡¨ç¤ºåè¿›åˆ¶æ•°å­—
print(f"å­—ç¬¦'!'çš„ç±»åˆ«: {unicodedata.category('!')}")  # Poè¡¨ç¤ºæ ‡ç‚¹ç¬¦å·
print(f"å­—ç¬¦' 'çš„ç±»åˆ«: {unicodedata.category(' ')}")  # Zsè¡¨ç¤ºç©ºç™½å­—ç¬¦
print(f"å­—ç¬¦'ä½ 'çš„ç±»åˆ«: {unicodedata.category('ä½ ')}")  # Loè¡¨ç¤ºå…¶ä»–å­—æ¯
print()

# ç±»åˆ«ç¼–ç å«ä¹‰ï¼š
# L: å­—æ¯ (Letter)
#   Lu: å¤§å†™å­—æ¯ (Uppercase letter)
#   Ll: å°å†™å­—æ¯ (Lowercase letter)
#   Lt: é¦–å­—æ¯å¤§å†™å­—æ¯ (Titlecase letter)
#   Lm: ä¿®é¥°å­—æ¯ (Modifier letter)
#   Lo: å…¶ä»–å­—æ¯ (Other letter)
# M: æ ‡è®° (Mark)
# N: æ•°å­— (Number)
#   Nd: åè¿›åˆ¶æ•°å­— (Decimal digit)
#   Nl: å­—æ¯æ•°å­— (Letter number)
#   No: å…¶ä»–æ•°å­— (Other number)
# P: æ ‡ç‚¹ç¬¦å· (Punctuation)
# S: ç¬¦å· (Symbol)
# Z: åˆ†éš”ç¬¦ (Separator)
# C: å…¶ä»– (Other)

# è·å–å­—ç¬¦çš„åè¿›åˆ¶æ•°å€¼
print(f"å­—ç¬¦'1'çš„æ•°å€¼: {unicodedata.numeric('1')}")
print(f"å­—ç¬¦'ä¹'çš„æ•°å€¼: {unicodedata.numeric('ä¹')}")
print(f"å­—ç¬¦'Â¾'çš„æ•°å€¼: {unicodedata.numeric('Â¾')}")
print(f"å­—ç¬¦'â…£'çš„æ•°å€¼: {unicodedata.numeric('â…£')}")  # ç½—é©¬æ•°å­—4
print()

# è·å–å­—ç¬¦çš„åè¿›åˆ¶æ•°å­—å€¼
print(f"å­—ç¬¦'1'çš„åè¿›åˆ¶æ•°å­—å€¼: {unicodedata.digit('1')}")
print(f"å­—ç¬¦'9'çš„åè¿›åˆ¶æ•°å­—å€¼: {unicodedata.digit('9')}")
print(f"å­—ç¬¦'â‚‰'çš„åè¿›åˆ¶æ•°å­—å€¼: {unicodedata.digit('â‚‰')}")  # ä¸‹æ ‡9
print()

# è·å–å­—ç¬¦çš„æ•´æ•°å€¼
print(f"å­—ç¬¦'1'çš„æ•´æ•°å€¼: {unicodedata.decimal('1')}")
print(f"å­—ç¬¦'5'çš„æ•´æ•°å€¼: {unicodedata.decimal('5')}")
print()

# 2. æ ¹æ®åç§°æŸ¥æ‰¾å­—ç¬¦
print("=== æ ¹æ®åç§°æŸ¥æ‰¾å­—ç¬¦ ===")

# ä½¿ç”¨lookupæ–¹æ³•æ ¹æ®åç§°æŸ¥æ‰¾å­—ç¬¦
print(f"åç§°ä¸º'GREEK SMALL LETTER PI'çš„å­—ç¬¦: {unicodedata.lookup('GREEK SMALL LETTER PI')}")
print(f"åç§°ä¸º'CHINESE CHARACTER FOR ONE'çš„å­—ç¬¦: {unicodedata.lookup('CHINESE CHARACTER FOR ONE')}")
print(f"åç§°ä¸º'PEACE SYMBOL'çš„å­—ç¬¦: {unicodedata.lookup('PEACE SYMBOL')}")
print(f"åç§°ä¸º'SMILING FACE WITH SMILING EYES'çš„å­—ç¬¦: {unicodedata.lookup('SMILING FACE WITH SMILING EYES')}")
print()

# 3. Unicodeè§„èŒƒåŒ–
print("=== Unicodeè§„èŒƒåŒ– ===")

# Unicodeè§„èŒƒåŒ–å½¢å¼:
# NFC: æ ‡å‡†ç­‰ä»·åˆæˆ (Canonical Composition)
# NFD: æ ‡å‡†ç­‰ä»·åˆ†è§£ (Canonical Decomposition)
# NFKC: å…¼å®¹ç­‰ä»·åˆæˆ (Compatibility Composition)
# NFKD: å…¼å®¹ç­‰ä»·åˆ†è§£ (Compatibility Decomposition)

# ç¤ºä¾‹ï¼šå¤„ç†å¸¦æœ‰é‡éŸ³ç¬¦å·çš„å­—ç¬¦
composed = 'Ã©'  # ç»„åˆå­—ç¬¦ e + Â´
decomposed = 'e\u0301'  # åˆ†è§£å½¢å¼ e + ç»„åˆé‡éŸ³ç¬¦

print(f"åŸå§‹ç»„åˆå­—ç¬¦: {repr(composed)} - é•¿åº¦: {len(composed)}")
print(f"åŸå§‹åˆ†è§£å­—ç¬¦: {repr(decomposed)} - é•¿åº¦: {len(decomposed)}")
print()

# NFCè§„èŒƒåŒ–ï¼ˆåˆæˆï¼‰
nfc_composed = unicodedata.normalize('NFC', composed)
nfc_decomposed = unicodedata.normalize('NFC', decomposed)
print(f"NFCè§„èŒƒåŒ–åçš„ç»„åˆå­—ç¬¦: {repr(nfc_composed)} - é•¿åº¦: {len(nfc_composed)}")
print(f"NFCè§„èŒƒåŒ–åçš„åˆ†è§£å­—ç¬¦: {repr(nfc_decomposed)} - é•¿åº¦: {len(nfc_decomposed)}")
print(f"NFCè§„èŒƒåŒ–åä¸¤è€…æ˜¯å¦ç›¸ç­‰: {nfc_composed == nfc_decomposed}")
print()

# NFDè§„èŒƒåŒ–ï¼ˆåˆ†è§£ï¼‰
nfd_composed = unicodedata.normalize('NFD', composed)
nfd_decomposed = unicodedata.normalize('NFD', decomposed)
print(f"NFDè§„èŒƒåŒ–åçš„ç»„åˆå­—ç¬¦: {repr(nfd_composed)} - é•¿åº¦: {len(nfd_composed)}")
print(f"NFDè§„èŒƒåŒ–åçš„åˆ†è§£å­—ç¬¦: {repr(nfd_decomposed)} - é•¿åº¦: {len(nfd_decomposed)}")
print(f"NFDè§„èŒƒåŒ–åä¸¤è€…æ˜¯å¦ç›¸ç­‰: {nfd_composed == nfd_decomposed}")
print()

# NFKCè§„èŒƒåŒ–ï¼ˆå…¼å®¹ç­‰ä»·åˆæˆï¼‰
# ç¤ºä¾‹ï¼šå¤„ç†å…¨è§’æ•°å­—
full_width = 'ï¼‘ï¼’ï¼“'  # å…¨è§’æ•°å­—
normal = '123'  # åŠè§’æ•°å­—

print(f"å…¨è§’æ•°å­—: {repr(full_width)}")
print(f"åŠè§’æ•°å­—: {repr(normal)}")
print(f"åŸå§‹æƒ…å†µä¸‹æ˜¯å¦ç›¸ç­‰: {full_width == normal}")
print()

nfkc_full = unicodedata.normalize('NFKC', full_width)
nfkc_normal = unicodedata.normalize('NFKC', normal)
print(f"NFKCè§„èŒƒåŒ–åçš„å…¨è§’æ•°å­—: {repr(nfkc_full)}")
print(f"NFKCè§„èŒƒåŒ–åçš„åŠè§’æ•°å­—: {repr(nfkc_normal)}")
print(f"NFKCè§„èŒƒåŒ–åä¸¤è€…æ˜¯å¦ç›¸ç­‰: {nfkc_full == nfkc_normal}")
print()

# NFKDè§„èŒƒåŒ–ï¼ˆå…¼å®¹ç­‰ä»·åˆ†è§£ï¼‰
nfkd_full = unicodedata.normalize('NFKD', full_width)
print(f"NFKDè§„èŒƒåŒ–åçš„å…¨è§’æ•°å­—: {repr(nfkd_full)}")
print()

# 4. å­—ç¬¦åˆ†è§£ä¿¡æ¯
print("=== å­—ç¬¦åˆ†è§£ä¿¡æ¯ ===")

# è·å–å­—ç¬¦çš„åˆ†è§£æ˜ å°„
print(f"å­—ç¬¦'Ã©'çš„åˆ†è§£æ˜ å°„: {repr(unicodedata.decomposition('Ã©'))}")
print(f"å­—ç¬¦'Ã±'çš„åˆ†è§£æ˜ å°„: {repr(unicodedata.decomposition('Ã±'))}")
print(f"å­—ç¬¦'â‚¬'çš„åˆ†è§£æ˜ å°„: {repr(unicodedata.decomposition('â‚¬'))}")
print(f"å­—ç¬¦'â…“'çš„åˆ†è§£æ˜ å°„: {repr(unicodedata.decomposition('â…“'))}")
print()

# 5. å…¶ä»–å­—ç¬¦å±æ€§
print("=== å…¶ä»–å­—ç¬¦å±æ€§ ===")

# è·å–å­—ç¬¦çš„åŒå‘ç±»åˆ«
print(f"å­—ç¬¦'A'çš„åŒå‘ç±»åˆ«: {unicodedata.bidirectional('A')}")
print(f"å­—ç¬¦'\u0644'çš„åŒå‘ç±»åˆ«: {unicodedata.bidirectional('\u0644')}")  # é˜¿æ‹‰ä¼¯å­—æ¯lam
print()

# è·å–å­—ç¬¦çš„é•œåƒå­—ç¬¦
print(f"å­—ç¬¦'('çš„é•œåƒå­—ç¬¦: {unicodedata.mirror('(')}")
print(f"å­—ç¬¦'A'çš„é•œåƒå­—ç¬¦: {unicodedata.mirror('A')}")  # éé•œåƒå­—ç¬¦è¿”å›None
print()

# æ£€æŸ¥å­—ç¬¦æ˜¯å¦æœ‰ç»„åˆæ ‡è®°
print(f"å­—ç¬¦'a'æ˜¯å¦æœ‰ç»„åˆæ ‡è®°: {unicodedata.combining('a')}")
print(f"å­—ç¬¦'\u0301'æ˜¯å¦æœ‰ç»„åˆæ ‡è®°: {unicodedata.combining('\u0301')}")  # ç»„åˆé‡éŸ³ç¬¦
print()

# è·å–East Asian Widthå±æ€§
print(f"å­—ç¬¦'A'çš„EAW: {unicodedata.east_asian_width('A')}")  # N: ä¸­æ€§
print(f"å­—ç¬¦'ä½ 'çš„EAW: {unicodedata.east_asian_width('ä½ ')}")  # W: å®½
print(f"å­—ç¬¦'ï½±'çš„EAW: {unicodedata.east_asian_width('ï½±')}")  # F: å…¨è§’
print()

# 6. å®é™…åº”ç”¨ç¤ºä¾‹
def practical_examples():
    """æ¼”ç¤ºå®é™…åº”ç”¨ç¤ºä¾‹"""
    print("=== å®é™…åº”ç”¨ç¤ºä¾‹ ===")
    
    # 1. æ–‡æœ¬è§„èŒƒåŒ–ä»¥è¿›è¡Œæ¯”è¾ƒ
    def normalize_text(text):
        """è§„èŒƒåŒ–æ–‡æœ¬ä»¥è¿›è¡Œæ¯”è¾ƒ"""
        # ä½¿ç”¨NFKCè§„èŒƒåŒ–ï¼Œå¤„ç†å„ç§å­—ç¬¦å˜ä½“
        return unicodedata.normalize('NFKC', text)
    
    # æµ‹è¯•ä¸åŒå½¢å¼çš„ç›¸åŒæ–‡æœ¬
    text_variants = [
        'cafÃ©',  # ç»„åˆå­—ç¬¦
        'cafe\u0301',  # åˆ†è§£å½¢å¼
        'CAFÃ‰',  # å¤§å†™å½¢å¼
        'ï½ƒï½ï½†ï½…ï¼‡',  # å…¨è§’å­—ç¬¦
    ]
    
    print("æ–‡æœ¬è§„èŒƒåŒ–ç¤ºä¾‹:")
    for i, text in enumerate(text_variants, 1):
        normalized = normalize_text(text)
        print(f"  å˜ä½“{i}: {repr(text)} -> è§„èŒƒåŒ–: {repr(normalized)}")
    
    # è§„èŒƒåŒ–åæ¯”è¾ƒ
    normalized_variants = [normalize_text(t).lower() for t in text_variants]
    print(f"è§„èŒƒåŒ–å¹¶è½¬å°å†™åï¼Œæ‰€æœ‰å˜ä½“æ˜¯å¦ç›¸ç­‰: {all(v == normalized_variants[0] for v in normalized_variants)}")
    print()
    
    # 2. è¯†åˆ«å’Œåˆ†ç±»å­—ç¬¦
    def categorize_text(text):
        """å¯¹æ–‡æœ¬ä¸­çš„å­—ç¬¦è¿›è¡Œåˆ†ç±»ç»Ÿè®¡"""
        categories = {}
        
        for char in text:
            cat = unicodedata.category(char)
            categories[cat] = categories.get(cat, 0) + 1
        
        return categories
    
    mixed_text = "Hello, ä¸–ç•Œ! 123 Ï€ = 3.14159..."
    categories = categorize_text(mixed_text)
    
    print("å­—ç¬¦åˆ†ç±»ç»Ÿè®¡:")
    for cat, count in categories.items():
        print(f"  ç±»åˆ« {cat}: {count}ä¸ªå­—ç¬¦")
    print()
    
    # 3. ç§»é™¤ç»„åˆé‡éŸ³ç¬¦å·
    def remove_diacritics(text):
        """ç§»é™¤æ–‡æœ¬ä¸­çš„æ‰€æœ‰ç»„åˆé‡éŸ³ç¬¦å·"""
        # é¦–å…ˆè¿›è¡ŒNFDåˆ†è§£ï¼Œå°†ç»„åˆå­—ç¬¦åˆ†è§£ä¸ºåŸºå­—ç¬¦å’Œé‡éŸ³ç¬¦å·
        # ç„¶åè¿‡æ»¤æ‰æ‰€æœ‰ç»„åˆæ ‡è®°å­—ç¬¦
        # æœ€åè¿›è¡ŒNFCé‡æ–°ç»„åˆ
        nfd_form = unicodedata.normalize('NFD', text)
        no_diacritics = ''.join([char for char in nfd_form if not unicodedata.combining(char)])
        return unicodedata.normalize('NFC', no_diacritics)
    
    accented_text = "CafÃ© rÃ©sumÃ© naÃ¯ve crÃ¨me brÃ»lÃ©e"
    text_without_diacritics = remove_diacritics(accented_text)
    
    print("ç§»é™¤é‡éŸ³ç¬¦å·ç¤ºä¾‹:")
    print(f"  åŸå§‹æ–‡æœ¬: {accented_text}")
    print(f"  å¤„ç†å: {text_without_diacritics}")
    print()
    
    # 4. å…¨è§’å’ŒåŠè§’å­—ç¬¦è½¬æ¢
    def to_half_width(text):
        """å°†å…¨è§’å­—ç¬¦è½¬æ¢ä¸ºåŠè§’å­—ç¬¦"""
        return unicodedata.normalize('NFKC', text)
    
    def is_full_width(char):
        """æ£€æŸ¥å­—ç¬¦æ˜¯å¦ä¸ºå…¨è§’å­—ç¬¦"""
        return unicodedata.east_asian_width(char) in ('F', 'W')
    
    mixed_width_text = "ï¼¡ï¼¢ï¼£abcï¼‘ï¼’ï¼“123ä½ å¥½"
    half_width_text = to_half_width(mixed_width_text)
    
    print("å…¨è§’è½¬åŠè§’ç¤ºä¾‹:")
    print(f"  åŸå§‹æ–‡æœ¬: {mixed_width_text}")
    print(f"  è½¬æ¢å: {half_width_text}")
    
    # æ£€æŸ¥å­—ç¬¦å®½åº¦
    print("  å­—ç¬¦å®½åº¦æ£€æŸ¥:")
    for char in mixed_width_text:
        width_type = "å…¨è§’" if is_full_width(char) else "åŠè§’"
        print(f"    '{char}': {width_type}")
    print()
    
    # 5. æ¸…ç†ç‰¹æ®Šå­—ç¬¦
    def clean_text(text, keep_categories=None):
        """æ¸…ç†æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™æŒ‡å®šç±»åˆ«çš„å­—ç¬¦"""
        if keep_categories is None:
            # é»˜è®¤ä¿ç•™å­—æ¯ã€æ•°å­—ã€æ ‡ç‚¹ç¬¦å·å’Œç©ºç™½å­—ç¬¦
            keep_categories = {'Lu', 'Ll', 'Lt', 'Lm', 'Lo', 'Nd', 'Nl', 'No', 'Pc', 'Pd', 'Ps', 'Pe', 'Pi', 'Pf', 'Po', 'Zs'}
        
        return ''.join([char for char in text if unicodedata.category(char) in keep_categories])
    
    dirty_text = "Hello!@#$%^&*() ä¸–ç•Œ123\t\n\ræµ‹è¯•\u0000\u0001ç‰¹æ®Šå­—ç¬¦"
    cleaned_text = clean_text(dirty_text)
    
    print("æ–‡æœ¬æ¸…ç†ç¤ºä¾‹:")
    print(f"  åŸå§‹æ–‡æœ¬: {repr(dirty_text)}")
    print(f"  æ¸…ç†å: {repr(cleaned_text)}")
    print()
    
    # 6. æ„å»ºè‡ªå®šä¹‰å­—ç¬¦æ˜ å°„
    def build_char_mapping(source_chars, target_chars):
        """æ„å»ºå­—ç¬¦æ˜ å°„å­—å…¸ï¼Œè€ƒè™‘Unicodeè§„èŒƒåŒ–"""
        mapping = {}
        
        for s_char, t_char in zip(source_chars, target_chars):
            # å¯¹æºå­—ç¬¦å’Œç›®æ ‡å­—ç¬¦è¿›è¡Œè§„èŒƒåŒ–
            s_norm = unicodedata.normalize('NFKC', s_char)
            t_norm = unicodedata.normalize('NFKC', t_char)
            mapping[s_norm] = t_norm
        
        return mapping
    
    def apply_mapping(text, mapping):
        """åº”ç”¨å­—ç¬¦æ˜ å°„"""
        result = []
        for char in text:
            norm_char = unicodedata.normalize('NFKC', char)
            result.append(mapping.get(norm_char, char))
        return ''.join(result)
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ˜ å°„ç¤ºä¾‹
    source = "abcABC"
    target = "123456"
    char_map = build_char_mapping(source, target)
    
    test_text = "abcABCï¼ŒaBc"
    mapped_text = apply_mapping(test_text, char_map)
    
    print("å­—ç¬¦æ˜ å°„ç¤ºä¾‹:")
    print(f"  æ˜ å°„è¡¨: {char_map}")
    print(f"  åŸå§‹æ–‡æœ¬: {test_text}")
    print(f"  æ˜ å°„å: {mapped_text}")
    print()
    
    # 7. è®¡ç®—æ–‡æœ¬çš„å®é™…æ˜¾ç¤ºå®½åº¦
    def get_display_width(text):
        """è®¡ç®—æ–‡æœ¬çš„å®é™…æ˜¾ç¤ºå®½åº¦ï¼ˆè€ƒè™‘å…¨è§’å­—ç¬¦ï¼‰"""
        width = 0
        for char in text:
            eaw = unicodedata.east_asian_width(char)
            # å…¨è§’å­—ç¬¦å®½åº¦ä¸º2ï¼Œå…¶ä»–ä¸º1
            width += 2 if eaw in ('F', 'W') else 1
        return width
    
    # æµ‹è¯•æ˜¾ç¤ºå®½åº¦
    width_test_cases = [
        "Hello World",
        "ä½ å¥½ï¼Œä¸–ç•Œ",
        "Hello ä¸–ç•Œ",
        "ï¼¡ï¼¢ï¼£abc",
    ]
    
    print("æ–‡æœ¬æ˜¾ç¤ºå®½åº¦è®¡ç®—:")
    for text in width_test_cases:
        width = get_display_width(text)
        print(f"  '{text}': æ˜¾ç¤ºå®½åº¦ = {width}")
    print()
    
    # 8. ç”ŸæˆUnicodeå­—ç¬¦è¡¨
    def generate_unicode_table(start_code, end_code, categories=None):
        """ç”ŸæˆUnicodeå­—ç¬¦è¡¨"""
        results = []
        for code in range(start_code, end_code + 1):
            char = chr(code)
            # è¿‡æ»¤ç±»åˆ«
            if categories and unicodedata.category(char) not in categories:
                continue
            
            try:
                name = unicodedata.name(char)
                category = unicodedata.category(char)
                results.append((char, code, name, category))
            except ValueError:
                # æœ‰äº›å­—ç¬¦å¯èƒ½æ²¡æœ‰åç§°
                pass
        return results
    
    # ç”Ÿæˆä¸€äº›å¸Œè…Šå­—æ¯
    greek_letters = generate_unicode_table(0x03B1, 0x03C9)  # å°å†™å¸Œè…Šå­—æ¯
    
    print("å¸Œè…Šå­—æ¯Unicodeè¡¨ï¼ˆéƒ¨åˆ†ï¼‰:")
    for char, code, name, category in greek_letters[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
        print(f"  {char} (U+{code:04X}) - {name} [{category}]")
    print()
    
    # 9. æ£€æŸ¥å­—ç¬¦æ˜¯å¦ä¸ºæ•°å­—
    def is_number(char):
        """æ£€æŸ¥å­—ç¬¦æ˜¯å¦ä¸ºæ•°å­—ï¼ˆåŒ…æ‹¬å„ç§Unicodeæ•°å­—ï¼‰"""
        try:
            # å°è¯•è·å–å­—ç¬¦çš„æ•°å€¼
            return unicodedata.numeric(char) is not None
        except (TypeError, ValueError):
            return False
    
    # æµ‹è¯•å„ç§æ•°å­—å­—ç¬¦
    number_test_cases = "12345 Â½ Â¾ â…“ â…” â…› â‘  â‘¡ ä¸‰ å›› ä¼ æ‹¾"
    
    print("æ•°å­—å­—ç¬¦æ£€æŸ¥:")
    for char in number_test_cases:
        if char.strip():
            is_num = is_number(char)
            numeric_value = unicodedata.numeric(char) if is_num else "N/A"
            print(f"  '{char}': æ˜¯æ•°å­— = {is_num}, æ•°å€¼ = {numeric_value}")
    print()
    
    # 10. è§„èŒƒåŒ–æ–‡ä»¶è·¯å¾„ï¼ˆå¤„ç†ä¸åŒå½¢å¼çš„å­—ç¬¦ï¼‰
    def normalize_path(path):
        """è§„èŒƒåŒ–æ–‡ä»¶è·¯å¾„ä¸­çš„å­—ç¬¦"""
        # ä½¿ç”¨NFKCè§„èŒƒåŒ–ï¼Œç¡®ä¿å…¨è§’å­—ç¬¦è½¬ä¸ºåŠè§’
        return unicodedata.normalize('NFKC', path)
    
    paths = [
        "C:\\Users\\User\\Documents\\å·¥ä½œ",
        "Cï¼š\\ï¼µï½“ï½…ï½’ï½“\\ï¼µï½“ï½…ï½’\\ï¼¤ï½ï½ƒï½•ï½ï½…ï½ï½”ï½“\\å·¥ä½œ"
    ]
    
    print("æ–‡ä»¶è·¯å¾„è§„èŒƒåŒ–:")
    for path in paths:
        normalized = normalize_path(path)
        print(f"  åŸå§‹è·¯å¾„: {path}")
        print(f"  è§„èŒƒåŒ–å: {normalized}")
    print()

# 7. é«˜çº§ç”¨æ³•å’ŒæŠ€å·§
print("=== é«˜çº§ç”¨æ³•å’ŒæŠ€å·§ ===")

# 1. è‡ªå®šä¹‰å­—ç¬¦è¿‡æ»¤å™¨
class UnicodeCharFilter:
    """Unicodeå­—ç¬¦è¿‡æ»¤å™¨"""
    
    def __init__(self, allowed_categories=None):
        self.allowed_categories = allowed_categories or {'Lu', 'Ll', 'Nd', 'Zs'}
    
    def filter(self, text):
        """è¿‡æ»¤æ–‡æœ¬ï¼Œåªä¿ç•™å…è®¸çš„å­—ç¬¦ç±»åˆ«"""
        return ''.join([char for char in text if unicodedata.category(char) in self.allowed_categories])
    
    def add_category(self, category):
        """æ·»åŠ å…è®¸çš„å­—ç¬¦ç±»åˆ«"""
        self.allowed_categories.add(category)
    
    def remove_category(self, category):
        """ç§»é™¤å…è®¸çš„å­—ç¬¦ç±»åˆ«"""
        if category in self.allowed_categories:
            self.allowed_categories.remove(category)

# ä½¿ç”¨è‡ªå®šä¹‰è¿‡æ»¤å™¨
filter = UnicodeCharFilter()
filtered_text = filter.filter("Hello, ä¸–ç•Œ! 123")
print(f"é»˜è®¤è¿‡æ»¤å™¨ï¼ˆå­—æ¯ã€æ•°å­—ã€ç©ºç™½ï¼‰: {filtered_text}")

filter.add_category('Po')  # æ·»åŠ æ ‡ç‚¹ç¬¦å·ç±»åˆ«
filtered_text_with_punct = filter.filter("Hello, ä¸–ç•Œ! 123")
print(f"æ·»åŠ æ ‡ç‚¹ç¬¦å·å: {filtered_text_with_punct}")
print()

# 2. æ–‡æœ¬ç›¸ä¼¼åº¦æ¯”è¾ƒï¼ˆåŸºäºè§„èŒƒåŒ–ï¼‰
def text_similarity(text1, text2):
    """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆç®€å•å®ç°ï¼‰"""
    # è§„èŒƒåŒ–ä¸¤ä¸ªæ–‡æœ¬
    norm1 = unicodedata.normalize('NFKC', text1).lower()
    norm2 = unicodedata.normalize('NFKC', text2).lower()
    
    # è®¡ç®—æœ€é•¿å…¬å…±å­åºåˆ—é•¿åº¦
    def lcs_length(s1, s2):
        m, n = len(s1), len(s2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if s1[i-1] == s2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        
        return dp[m][n]
    
    lcs = lcs_length(norm1, norm2)
    max_len = max(len(norm1), len(norm2))
    
    # è¿”å›ç›¸ä¼¼åº¦ï¼ˆ0-1ä¹‹é—´ï¼‰
    return lcs / max_len if max_len > 0 else 1.0

# æµ‹è¯•æ–‡æœ¬ç›¸ä¼¼åº¦
text1 = "CafÃ© au lait"
text2 = "cafÃ©aulait"
text3 = "cafe au lait"
text4 = "coffee with milk"

sim12 = text_similarity(text1, text2)
sim13 = text_similarity(text1, text3)
sim14 = text_similarity(text1, text4)

print("æ–‡æœ¬ç›¸ä¼¼åº¦æ¯”è¾ƒ:")
print(f"'{text1}' ä¸ '{text2}' çš„ç›¸ä¼¼åº¦: {sim12:.4f}")
print(f"'{text1}' ä¸ '{text3}' çš„ç›¸ä¼¼åº¦: {sim13:.4f}")
print(f"'{text1}' ä¸ '{text4}' çš„ç›¸ä¼¼åº¦: {sim14:.4f}")
print()

# 3. Unicodeæ–‡æœ¬åˆ†è¯å™¨ï¼ˆç®€å•å®ç°ï¼‰
def unicode_tokenizer(text):
    """ç®€å•çš„Unicodeæ–‡æœ¬åˆ†è¯å™¨"""
    tokens = []
    current_token = ""
    current_category = None
    
    for char in text:
        cat = unicodedata.category(char)
        # åŸºæœ¬åˆ†ç±»ï¼šå­—æ¯ã€æ•°å­—ã€å…¶ä»–
        if cat.startswith('L'):
            new_category = 'letter'
        elif cat.startswith('N'):
            new_category = 'number'
        else:
            new_category = 'other'
        
        if new_category == current_category or current_token == "":
            # ç›¸åŒç±»åˆ«ï¼Œç»§ç»­æ·»åŠ 
            current_token += char
            current_category = new_category
        else:
            # ä¸åŒç±»åˆ«ï¼Œä¿å­˜å½“å‰token
            if current_token and current_category != 'other':
                tokens.append(current_token)
            current_token = char if new_category != 'other' else ""
            current_category = new_category
    
    # æ·»åŠ æœ€åä¸€ä¸ªtoken
    if current_token and current_category != 'other':
        tokens.append(current_token)
    
    return tokens

# æµ‹è¯•åˆ†è¯å™¨
mixed_text = "Hello, ä¸–ç•Œ! 123-test_æ–‡æœ¬."
tokens = unicode_tokenizer(mixed_text)
print(f"åˆ†è¯ç»“æœ: {tokens}")
print()

# 4. å­—ç¬¦å±æ€§ç¼“å­˜ï¼ˆæé«˜æ€§èƒ½ï¼‰
class UnicodePropertyCache:
    """Unicodeå­—ç¬¦å±æ€§ç¼“å­˜"""
    
    def __init__(self):
        self.category_cache = {}
        self.name_cache = {}
        self.numeric_cache = {}
    
    def get_category(self, char):
        """è·å–å­—ç¬¦ç±»åˆ«ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        if char not in self.category_cache:
            self.category_cache[char] = unicodedata.category(char)
        return self.category_cache[char]
    
    def get_name(self, char):
        """è·å–å­—ç¬¦åç§°ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        if char not in self.name_cache:
            try:
                self.name_cache[char] = unicodedata.name(char)
            except ValueError:
                self.name_cache[char] = None
        return self.name_cache[char]
    
    def get_numeric(self, char):
        """è·å–å­—ç¬¦æ•°å€¼ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        if char not in self.numeric_cache:
            try:
                self.numeric_cache[char] = unicodedata.numeric(char)
            except (TypeError, ValueError):
                self.numeric_cache[char] = None
        return self.numeric_cache[char]
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.category_cache.clear()
        self.name_cache.clear()
        self.numeric_cache.clear()

# ä½¿ç”¨å±æ€§ç¼“å­˜
cache = UnicodePropertyCache()

# æµ‹è¯•ç¼“å­˜æ€§èƒ½
import time

def test_without_cache(text):
    """ä¸ä½¿ç”¨ç¼“å­˜æµ‹è¯•"""
    start = time.time()
    for _ in range(1000):
        for char in text:
            unicodedata.category(char)
    return time.time() - start

def test_with_cache(text, cache):
    """ä½¿ç”¨ç¼“å­˜æµ‹è¯•"""
    start = time.time()
    for _ in range(1000):
        for char in text:
            cache.get_category(char)
    return time.time() - start

# åˆ›å»ºæµ‹è¯•æ–‡æœ¬
import string
test_text = string.ascii_letters + string.digits + "ä½ å¥½ï¼Œä¸–ç•Œï¼" * 10

time_without = test_without_cache(test_text)
time_with = test_with_cache(test_text, cache)

print("æ€§èƒ½æµ‹è¯•ï¼ˆè·å–å­—ç¬¦ç±»åˆ«ï¼‰:")
print(f"ä¸ä½¿ç”¨ç¼“å­˜: {time_without:.6f}ç§’")
print(f"ä½¿ç”¨ç¼“å­˜: {time_with:.6f}ç§’")
print(f"æ€§èƒ½æå‡: {(time_without/time_with):.2f}å€")
print()

# 8. æ³¨æ„äº‹é¡¹å’Œæœ€ä½³å®è·µ
"""
1. **è§„èŒƒåŒ–é€‰æ‹©**ï¼š
   - NFCï¼šç”¨äºä¸€èˆ¬æ–‡æœ¬å­˜å‚¨å’Œæ˜¾ç¤ºï¼Œä¿ç•™ç»„åˆå­—ç¬¦
   - NFDï¼šç”¨äºæ–‡æœ¬å¤„ç†ï¼Œåˆ†è§£å­—ç¬¦ä¾¿äºæ“ä½œå•ä¸ªæ ‡è®°
   - NFKCï¼šç”¨äºæœç´¢ã€ç´¢å¼•å’Œæ¯”è¾ƒï¼Œå¤„ç†å…¼å®¹å­—ç¬¦
   - NFKDï¼šç”¨äºéœ€è¦å®Œå…¨åˆ†è§£çš„åœºæ™¯

2. **æ€§èƒ½è€ƒè™‘**ï¼š
   - é¢‘ç¹è°ƒç”¨unicodedataå‡½æ•°å¯èƒ½å½±å“æ€§èƒ½ï¼Œè€ƒè™‘ä½¿ç”¨ç¼“å­˜
   - å¯¹äºå¤§é‡æ–‡æœ¬å¤„ç†ï¼Œå¯ä»¥æ‰¹é‡è§„èŒƒåŒ–

3. **å­—ç¬¦ä¸²æ¯”è¾ƒ**ï¼š
   - åœ¨æ¯”è¾ƒå­—ç¬¦ä¸²æ—¶ï¼Œå§‹ç»ˆå…ˆè¿›è¡Œè§„èŒƒåŒ–å¤„ç†
   - ä¸åŒå½¢å¼çš„ç›¸åŒæ–‡æœ¬ï¼ˆå¦‚å¸¦é‡éŸ³ç¬¦å·çš„å˜ä½“ï¼‰éœ€è¦è§„èŒƒåŒ–åå†æ¯”è¾ƒ

4. **æ–‡æœ¬æ¸…æ´—**ï¼š
   - ä½¿ç”¨å­—ç¬¦ç±»åˆ«è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„æ–‡æœ¬æ¸…æ´—
   - æ³¨æ„ä¿ç•™éœ€è¦çš„ç‰¹æ®Šå­—ç¬¦

5. **å›½é™…åŒ–æ”¯æŒ**ï¼š
   - è€ƒè™‘ä¸åŒè¯­è¨€çš„å­—ç¬¦ç‰¹æ€§
   - æ³¨æ„å³åˆ°å·¦è¯­è¨€çš„åŒå‘æ–‡æœ¬å¤„ç†

6. **æ˜¾ç¤ºå®½åº¦**ï¼š
   - åœ¨éœ€è¦ç²¾ç¡®è®¡ç®—æ–‡æœ¬æ˜¾ç¤ºå®½åº¦æ—¶ï¼Œè€ƒè™‘ä½¿ç”¨east_asian_width
   - å…¨è§’å­—ç¬¦é€šå¸¸å ç”¨ä¸¤å€å®½åº¦

7. **é”™è¯¯å¤„ç†**ï¼š
   - æŸäº›å­—ç¬¦å¯èƒ½æ²¡æœ‰åç§°æˆ–æ•°å€¼ï¼Œéœ€è¦æ•è·ValueErrorå¼‚å¸¸
   - ä½¿ç”¨try/exceptå—å¤„ç†å¯èƒ½çš„å¼‚å¸¸

8. **ç‰ˆæœ¬å…¼å®¹æ€§**ï¼š
   - Unicodeæ ‡å‡†åœ¨ä¸æ–­æ›´æ–°ï¼Œä¸åŒPythonç‰ˆæœ¬æ”¯æŒçš„Unicodeç‰ˆæœ¬å¯èƒ½ä¸åŒ
   - å¯¹äºç‰¹å®šå­—ç¬¦æˆ–å±æ€§ï¼Œéœ€è¦è€ƒè™‘ç‰ˆæœ¬å…¼å®¹æ€§
"""

def demonstrate_best_practices():
    """æ¼”ç¤ºæœ€ä½³å®è·µ"""
    print("=== æœ€ä½³å®è·µç¤ºä¾‹ ===")
    
    # 1. æ­£ç¡®çš„å­—ç¬¦ä¸²æ¯”è¾ƒ
    print("æ­£ç¡®çš„å­—ç¬¦ä¸²æ¯”è¾ƒ:")
    
    # ä¸¤ä¸ªçœ‹èµ·æ¥ç›¸åŒä½†å†…éƒ¨è¡¨ç¤ºä¸åŒçš„å­—ç¬¦ä¸²
    str1 = "cafÃ©"  # Ã©ä½œä¸ºå•ä¸ªå­—ç¬¦
    str2 = "cafe\u0301"  # eåè·Ÿç»„åˆé‡éŸ³ç¬¦
    
    print(f"å­—ç¬¦ä¸²1: {repr(str1)}, é•¿åº¦: {len(str1)}")
    print(f"å­—ç¬¦ä¸²2: {repr(str2)}, é•¿åº¦: {len(str2)}")
    print(f"ç›´æ¥æ¯”è¾ƒç»“æœ: {str1 == str2}")  # False
    
    # è§„èŒƒåŒ–åæ¯”è¾ƒ
    norm1 = unicodedata.normalize('NFC', str1)
    norm2 = unicodedata.normalize('NFC', str2)
    print(f"NFCè§„èŒƒåŒ–åæ¯”è¾ƒ: {norm1 == norm2}")  # True
    print()
    
    # 2. å¤„ç†æ–‡ä»¶ç³»ç»Ÿè·¯å¾„æ—¶çš„è§„èŒƒåŒ–
    print("æ–‡ä»¶ç³»ç»Ÿè·¯å¾„è§„èŒƒåŒ–:")
    
    def safe_path_compare(path1, path2):
        """å®‰å…¨æ¯”è¾ƒæ–‡ä»¶è·¯å¾„"""
        # ä½¿ç”¨NFKCè§„èŒƒåŒ–ï¼Œå¤„ç†å…¨è§’å­—ç¬¦ç­‰
        norm_path1 = unicodedata.normalize('NFKC', path1)
        norm_path2 = unicodedata.normalize('NFKC', path2)
        # è½¬æ¢ä¸ºå°å†™è¿›è¡Œå¤§å°å†™ä¸æ•æ„Ÿçš„æ¯”è¾ƒ
        return norm_path1.lower() == norm_path2.lower()
    
    path1 = "C:\\Users\\user\\æ–‡æ¡£"
    path2 = "cï¼š\\ï½•ï½“ï½…ï½’ï½“\\ï¼µï¼³ï¼¥ï¼²\\æ–‡æ¡£"
    
    print(f"è·¯å¾„1: {path1}")
    print(f"è·¯å¾„2: {path2}")
    print(f"ç›´æ¥æ¯”è¾ƒ: {path1 == path2}")  # False
    print(f"å®‰å…¨æ¯”è¾ƒ: {safe_path_compare(path1, path2)}")  # True
    print()
    
    # 3. æ–‡æœ¬æœç´¢æ—¶çš„è§„èŒƒåŒ–
    print("æ–‡æœ¬æœç´¢æ—¶çš„è§„èŒƒåŒ–:")
    
    def normalize_for_search(text):
        """ä¸ºæœç´¢å‡†å¤‡æ–‡æœ¬"""
        # ä½¿ç”¨NFKCè§„èŒƒåŒ–ï¼Œå¤„ç†å„ç§å…¼å®¹å­—ç¬¦
        # è½¬æ¢ä¸ºå°å†™
        # ç§»é™¤ç»„åˆé‡éŸ³ç¬¦å·
        text = unicodedata.normalize('NFKD', text)
        text = text.lower()
        text = ''.join([c for c in text if not unicodedata.combining(c)])
        return unicodedata.normalize('NFC', text)
    
    def search_text(text, query):
        """æœç´¢æ–‡æœ¬ï¼Œè€ƒè™‘Unicodeå˜ä½“"""
        norm_text = normalize_for_search(text)
        norm_query = normalize_for_search(query)
        return norm_query in norm_text
    
    document = "CafÃ© rÃ©sumÃ© et CrÃ¨me BrÃ»lÃ©e sont des mots franÃ§ais."
    queries = ["cafe", "CAFE", "cafÃ©", "RESUME", "brÃ»lÃ©e"]
    
    print(f"æ–‡æ¡£: {document}")
    print("æœç´¢ç»“æœ:")
    for q in queries:
        found = search_text(document, q)
        print(f"  '{q}': {'æ‰¾åˆ°' if found else 'æœªæ‰¾åˆ°'}")
    print()
    
    # 4. å®‰å…¨çš„å­—ç¬¦å±æ€§è®¿é—®
    print("å®‰å…¨çš„å­—ç¬¦å±æ€§è®¿é—®:")
    
    def safe_get_name(char):
        """å®‰å…¨è·å–å­—ç¬¦åç§°"""
        try:
            return unicodedata.name(char)
        except ValueError:
            return f"<æ— åç§°å­—ç¬¦: U+{ord(char):04X}>"
    
    def safe_get_numeric(char):
        """å®‰å…¨è·å–å­—ç¬¦æ•°å€¼"""
        try:
            return unicodedata.numeric(char)
        except (TypeError, ValueError):
            return None
    
    # æµ‹è¯•ä¸€äº›ç‰¹æ®Šå­—ç¬¦
    test_chars = ["A", "Ï€", "ä½ ", "\u0000", "\u001F", "ğŸ¤”"]
    
    print("å­—ç¬¦å±æ€§å®‰å…¨è®¿é—®:")
    for char in test_chars:
        name = safe_get_name(char)
        numeric = safe_get_numeric(char)
        print(f"  '{char}' (U+{ord(char):04X}): åç§° = {name}, æ•°å€¼ = {numeric}")
    print()
    
    # 5. å¤„ç†åŒå‘æ–‡æœ¬
    print("å¤„ç†åŒå‘æ–‡æœ¬:")
    
    def analyze_bidi_text(text):
        """åˆ†æåŒå‘æ–‡æœ¬"""
        result = []
        for char in text:
            bidi = unicodedata.bidirectional(char)
            result.append((char, bidi))
        return result
    
    # åŒ…å«é˜¿æ‹‰ä¼¯æ–‡å’Œè‹±æ–‡çš„æ··åˆæ–‡æœ¬
    bidi_text = "Hello Ù…Ø±Ø­Ø¨Ø§ World Ø¹Ø§Ù„Ù…"
    bidi_analysis = analyze_bidi_text(bidi_text)
    
    print(f"åŒå‘æ–‡æœ¬åˆ†æ: '{bidi_text}'")
    print("å­—ç¬¦åŒå‘ç±»åˆ«:")
    for char, bidi in bidi_analysis:
        print(f"  '{char}': {bidi}")
    print()
    
    # 6. æ¸…ç†ç”¨æˆ·è¾“å…¥
    print("æ¸…ç†ç”¨æˆ·è¾“å…¥:")
    
    def sanitize_input(text):
        """æ¸…ç†ç”¨æˆ·è¾“å…¥æ–‡æœ¬"""
        # 1. è§„èŒƒåŒ–æ–‡æœ¬
        text = unicodedata.normalize('NFKC', text)
        
        # 2. ç§»é™¤æ§åˆ¶å­—ç¬¦ï¼ˆä¿ç•™æ¢è¡Œç¬¦å’Œåˆ¶è¡¨ç¬¦ï¼‰
        sanitized = []
        for char in text:
            cat = unicodedata.category(char)
            # ç§»é™¤æ§åˆ¶å­—ç¬¦ï¼ˆCç±»ï¼‰ï¼Œä½†ä¿ç•™æ¢è¡Œå’Œåˆ¶è¡¨ç¬¦
            if cat.startswith('C') and ord(char) not in (9, 10, 13):
                continue
            sanitized.append(char)
        
        return ''.join(sanitized)
    
    # åŒ…å«æ§åˆ¶å­—ç¬¦å’Œå…¨è§’å­—ç¬¦çš„è¾“å…¥
    dirty_input = "User\u0007Input\tåŒ…å«\u0001æ§åˆ¶å­—ç¬¦å’Œå…¨è§’ABC"
    clean_input = sanitize_input(dirty_input)
    
    print(f"åŸå§‹è¾“å…¥: {repr(dirty_input)}")
    print(f"æ¸…ç†å: {repr(clean_input)}")
    print()

# è¿è¡Œæ¼”ç¤ºä»£ç 
if __name__ == "__main__":
    print("Python unicodedataæ¨¡å—æ¼”ç¤º\n")
    
    # è¿è¡ŒåŸºæœ¬æ¼”ç¤º
    # å®é™…åº”ç”¨ç¤ºä¾‹
    practical_examples()
    # é«˜çº§ç”¨æ³•å’Œæœ€ä½³å®è·µ
    demonstrate_best_practices()
    
    print("æ¼”ç¤ºå®Œæˆï¼")