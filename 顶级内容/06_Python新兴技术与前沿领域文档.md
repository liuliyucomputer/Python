# Pythonæ–°å…´æŠ€æœ¯ä¸å‰æ²¿é¢†åŸŸæ–‡æ¡£

## å‰è¨€

éšç€æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼ŒPythonåœ¨æ–°å…´é¢†åŸŸçš„åº”ç”¨ä¹Ÿè¶Šæ¥è¶Šå¹¿æ³›ã€‚æœ¬æ–‡æ¡£å°†æ·±å…¥æ¢è®¨Pythonåœ¨é‡å­è®¡ç®—ã€è¾¹ç¼˜AIã€ä½ä»£ç å¼€å‘ç­‰å‰æ²¿é¢†åŸŸçš„åº”ç”¨ä¸å®è·µï¼Œå¸®åŠ©å¼€å‘è€…äº†è§£æœ€æ–°çš„æŠ€æœ¯è¶‹åŠ¿å’Œå‘å±•æ–¹å‘ã€‚

## 1. é‡å­è®¡ç®—ä¸é‡å­ç®—æ³•

### 1.1 é‡å­è®¡ç®—åŸºç¡€æ¦‚å¿µ
**[æ ‡è¯†: QUANTUM-001]**

é‡å­è®¡ç®—æ˜¯ä¸€ç§åŸºäºé‡å­åŠ›å­¦åŸç†çš„è®¡ç®—æ–¹å¼ï¼Œå…·æœ‰ä¼ ç»Ÿè®¡ç®—æ— æ³•æ¯”æ‹Ÿçš„ä¼˜åŠ¿ï¼š

- **é‡å­æ¯”ç‰¹(Qubit)**: é‡å­è®¡ç®—çš„åŸºæœ¬å•ä½ï¼Œå¯ä»¥åŒæ—¶å¤„äº0å’Œ1çš„å åŠ æ€
- **é‡å­çº ç¼ **: å¤šä¸ªé‡å­æ¯”ç‰¹ä¹‹é—´å­˜åœ¨çš„ç‰¹æ®Šå…³è”å…³ç³»
- **é‡å­å¹¶è¡Œ**: åŒæ—¶å¤„ç†å¤šä¸ªè®¡ç®—çŠ¶æ€çš„èƒ½åŠ›
- **é‡å­é—¨**: é‡å­è®¡ç®—ä¸­çš„åŸºæœ¬æ“ä½œå•å…ƒ

### 1.2 Pythoné‡å­è®¡ç®—æ¡†æ¶
**[æ ‡è¯†: QUANTUM-002]**

Pythonæä¾›äº†å¤šä¸ªå¼ºå¤§çš„é‡å­è®¡ç®—æ¡†æ¶ï¼Œæ–¹ä¾¿å¼€å‘è€…è¿›è¡Œé‡å­ç®—æ³•ç ”ç©¶å’Œåº”ç”¨å¼€å‘ï¼š

#### Qiskit
IBMå¼€å‘çš„å¼€æºé‡å­è®¡ç®—æ¡†æ¶ï¼Œæä¾›äº†å®Œæ•´çš„é‡å­è®¡ç®—å¼€å‘ç”Ÿæ€ç³»ç»Ÿï¼š

```python
# QiskitåŸºæœ¬ä½¿ç”¨ç¤ºä¾‹
from qiskit import QuantumCircuit, Aer, execute
from qiskit.visualization import plot_histogram

# åˆ›å»ºé‡å­ç”µè·¯ï¼ˆ1ä¸ªé‡å­æ¯”ç‰¹ï¼Œ1ä¸ªç»å…¸æ¯”ç‰¹ï¼‰
qc = QuantumCircuit(1, 1)

# æ·»åŠ Hadamardé—¨ï¼Œåˆ›å»ºå åŠ æ€
qc.h(0)

# æ·»åŠ æµ‹é‡æ“ä½œ
qc.measure(0, 0)

# ç»˜åˆ¶é‡å­ç”µè·¯å›¾
print("é‡å­ç”µè·¯å›¾:")
print(qc.draw())

# ä½¿ç”¨æ¨¡æ‹Ÿå™¨æ‰§è¡Œç”µè·¯
backend = Aer.get_backend('qasm_simulator')
job = execute(qc, backend, shots=1000)
result = job.result()

# è·å–æµ‹é‡ç»“æœ
counts = result.get_counts(qc)
print("æµ‹é‡ç»“æœ:", counts)

# ç»˜åˆ¶ç»“æœç›´æ–¹å›¾
plot_histogram(counts)
```

#### Cirq
Googleå¼€å‘çš„é‡å­è®¡ç®—æ¡†æ¶ï¼Œä¸“æ³¨äºNISQ(Noisy Intermediate-Scale Quantum)è®¾å¤‡ï¼š

```python
# CirqåŸºæœ¬ä½¿ç”¨ç¤ºä¾‹
import cirq

# åˆ›å»ºé‡å­æ¯”ç‰¹
qubit = cirq.LineQubit(0)

# åˆ›å»ºé‡å­ç”µè·¯
circuit = cirq.Circuit()

# æ·»åŠ Hadamardé—¨
circuit.append(cirq.H(qubit))

# æ·»åŠ æµ‹é‡æ“ä½œ
circuit.append(cirq.measure(qubit, key='result'))

# ç»˜åˆ¶é‡å­ç”µè·¯å›¾
print("é‡å­ç”µè·¯å›¾:")
print(circuit)

# ä½¿ç”¨æ¨¡æ‹Ÿå™¨æ‰§è¡Œç”µè·¯
simulator = cirq.Simulator()
result = simulator.run(circuit, repetitions=1000)

# æ‰“å°ç»“æœ
counts = result.histogram(key='result')
print("æµ‹é‡ç»“æœ:", counts)
```

#### PennyLane
ä¸“æ³¨äºé‡å­æœºå™¨å­¦ä¹ çš„Pythonåº“ï¼Œæ”¯æŒé‡å­ç¥ç»ç½‘ç»œå’Œé‡å­ä¼˜åŒ–ï¼š

```python
# PennyLaneåŸºæœ¬ä½¿ç”¨ç¤ºä¾‹
import pennylane as qml
import numpy as np

# åˆ›å»ºé‡å­è®¾å¤‡
dev = qml.device("default.qubit", wires=1)

# å®šä¹‰é‡å­ç”µè·¯ä½œä¸ºé‡å­èŠ‚ç‚¹
@qml.qnode(dev)

def circuit(phi):
    qml.RX(phi, wires=0)
    return qml.expval(qml.PauliZ(0))

# è®¡ç®—æœŸæœ›å€¼
phi = np.pi/4
result = circuit(phi)
print(f"å½“phi = {phi:.3f}æ—¶ï¼ŒZçš„æœŸæœ›å€¼ä¸º: {result:.3f}")
```

### 1.3 é‡å­ç®—æ³•å®ç°
**[æ ‡è¯†: QUANTUM-003]**

Pythonå¯ä»¥å®ç°å„ç§ç»å…¸é‡å­ç®—æ³•ï¼Œå±•ç¤ºé‡å­è®¡ç®—çš„ä¼˜åŠ¿ï¼š

#### Groveræœç´¢ç®—æ³•
ç”¨äºåœ¨æ— åºæ•°æ®åº“ä¸­è¿›è¡Œå¿«é€Ÿæœç´¢çš„é‡å­ç®—æ³•ï¼š

```python
# Groveræœç´¢ç®—æ³•å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰
import numpy as np
from qiskit import QuantumCircuit, Aer, execute
from qiskit.visualization import plot_histogram

# åˆ›å»ºGroveræœç´¢ç”µè·¯
def create_grover_circuit(n_qubits, marked_state):
    qc = QuantumCircuit(n_qubits, n_qubits)
    
    # åˆå§‹åŒ–ï¼šåˆ›å»ºå‡ç­‰å åŠ æ€
    for q in range(n_qubits):
        qc.h(q)
    
    # Oracle: æ ‡è®°ç‰¹å®šçŠ¶æ€
    oracle = QuantumCircuit(n_qubits)
    # å®ç°ç›¸ä½ç¿»è½¬
    for i, bit in enumerate(marked_state):
        if bit == '0':
            oracle.x(i)
    # ä½¿ç”¨å¤šæ§Zé—¨
    oracle.h(n_qubits-1)
    oracle.mct(list(range(n_qubits-1)), n_qubits-1)
    oracle.h(n_qubits-1)
    # å†æ¬¡ç¿»è½¬
    for i, bit in enumerate(marked_state):
        if bit == '0':
            oracle.x(i)
    
    # Diffusion operator
    diff = QuantumCircuit(n_qubits)
    for q in range(n_qubits):
        diff.h(q)
    for q in range(n_qubits):
        diff.x(q)
    diff.h(n_qubits-1)
    diff.mct(list(range(n_qubits-1)), n_qubits-1)
    diff.h(n_qubits-1)
    for q in range(n_qubits):
        diff.x(q)
    for q in range(n_qubits):
        diff.h(q)
    
    # ç»„åˆç”µè·¯
    # Groverè¿­ä»£æ¬¡æ•°
    iterations = int(np.floor(np.pi/4 * np.sqrt(2**n_qubits)))
    
    for _ in range(iterations):
        qc.compose(oracle, inplace=True)
        qc.compose(diff, inplace=True)
    
    # æµ‹é‡
    qc.measure(range(n_qubits), range(n_qubits))
    
    return qc

# ç¤ºä¾‹ï¼šåœ¨2ä¸ªé‡å­æ¯”ç‰¹ä¸­æœç´¢æ ‡è®°çŠ¶æ€'11'
n_qubits = 2
marked_state = '11'

# åˆ›å»ºç”µè·¯
qc = create_grover_circuit(n_qubits, marked_state)
print("Groveræœç´¢ç”µè·¯å›¾:")
print(qc.draw())

# æ‰§è¡Œç”µè·¯
backend = Aer.get_backend('qasm_simulator')
job = execute(qc, backend, shots=1000)
result = job.result()
counts = result.get_counts(qc)

# æ‰“å°ç»“æœ
print("æœç´¢ç»“æœ:", counts)

# ç»˜åˆ¶ç›´æ–¹å›¾
plot_histogram(counts)
```

#### Shorç®—æ³•
ç”¨äºæ•´æ•°åˆ†è§£çš„é‡å­ç®—æ³•ï¼Œå¯¹å¯†ç å­¦æœ‰é‡è¦å½±å“ï¼š

```python
# Shorç®—æ³•çš„é‡å­éƒ¨åˆ†ï¼ˆQFTå®ç°ï¼‰
from qiskit import QuantumCircuit, QuantumRegister, ClassicalRegister
from qiskit.visualization import plot_histogram
from qiskit import Aer, execute
import numpy as np

def qft(circuit, qr, n):
    """
    å®ç°é‡å­å‚…é‡Œå¶å˜æ¢
    """
    for j in range(n):
        circuit.h(qr[j])
        for k in range(j+1, n):
            circuit.cp(np.pi/float(2**(k-j)), qr[k], qr[j])
    
    # åè½¬é‡å­æ¯”ç‰¹é¡ºåº
    for i in range(n//2):
        circuit.swap(qr[i], qr[n-i-1])

# åˆ›å»ºShorç®—æ³•çš„é‡å­ç”µè·¯ï¼ˆé’ˆå¯¹å‡½æ•°f(x) = a^x mod Nçš„å‘¨æœŸæŸ¥æ‰¾ï¼‰
def shor_circuit(a, N, n_count):
    # åˆ›å»ºå¯„å­˜å™¨
    qr = QuantumRegister(n_count)
    cr = ClassicalRegister(n_count)
    qc = QuantumCircuit(qr, cr)
    
    # åˆå§‹åŒ–ç¬¬ä¸€ä¸ªå¯„å­˜å™¨ä¸ºå åŠ æ€
    for i in range(n_count):
        qc.h(qr[i])
    
    # å®ç°å—æ§Uæ“ä½œï¼ˆç®€åŒ–ç‰ˆï¼Œå®Œæ•´å®ç°éœ€è¦QFTå’Œé‡å­ç›¸ä½ä¼°è®¡ï¼‰
    # æ³¨æ„ï¼šè¿™é‡Œåªå±•ç¤ºæ¡†æ¶ï¼Œå®Œæ•´Shorç®—æ³•éœ€è¦æ›´å¤æ‚çš„å®ç°
    
    # åº”ç”¨QFT
    qft(qc, qr, n_count)
    
    # æµ‹é‡
    qc.measure(range(n_count), range(n_count))
    
    return qc

# ç¤ºä¾‹ï¼šå¯»æ‰¾f(x) = 2^x mod 15çš„å‘¨æœŸ
n_count = 4  # è®¡æ•°é‡å­æ¯”ç‰¹æ•°é‡
a = 2
N = 15

# åˆ›å»ºç”µè·¯
qc = shor_circuit(a, N, n_count)
print("Shorç®—æ³•ç”µè·¯å›¾:")
print(qc.draw())

# æ‰§è¡Œç”µè·¯
backend = Aer.get_backend('qasm_simulator')
job = execute(qc, backend, shots=1000)
result = job.result()
counts = result.get_counts(qc)

# æ‰“å°ç»“æœ
print("æµ‹é‡ç»“æœ:", counts)
```

### 1.4 é‡å­è®¡ç®—åº”ç”¨å‰æ™¯
**[æ ‡è¯†: QUANTUM-004]**

é‡å­è®¡ç®—åœ¨å¤šä¸ªé¢†åŸŸå±•ç°å‡ºå·¨å¤§çš„åº”ç”¨æ½œåŠ›ï¼š

- **å¯†ç å­¦**: ç ´è§£RSAç­‰å…¬é’¥åŠ å¯†ç®—æ³•ï¼Œå¼€å‘æŠ—é‡å­å¯†ç ç³»ç»Ÿ
- **ä¼˜åŒ–é—®é¢˜**: è§£å†³æ—…è¡Œå•†é—®é¢˜ã€ç‰©æµè°ƒåº¦ç­‰ç»„åˆä¼˜åŒ–éš¾é¢˜
- **ææ–™ç§‘å­¦**: æ¨¡æ‹Ÿåˆ†å­ç»“æ„å’Œææ–™ç‰¹æ€§
- **é‡‘èå»ºæ¨¡**: é£é™©è¯„ä¼°å’ŒæŠ•èµ„ç»„åˆä¼˜åŒ–
- **æœºå™¨å­¦ä¹ **: é‡å­æœºå™¨å­¦ä¹ ç®—æ³•çš„å¼€å‘å’Œåº”ç”¨

## 2. è¾¹ç¼˜AIä¸ç‰©è”ç½‘

### 2.1 è¾¹ç¼˜è®¡ç®—ä¸AIç»“åˆ
**[æ ‡è¯†: EDGE-001]**

è¾¹ç¼˜AIæ˜¯æŒ‡åœ¨ç½‘ç»œè¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²AIæ¨¡å‹ï¼Œå®ç°æœ¬åœ°æ•°æ®å¤„ç†å’Œæ™ºèƒ½å†³ç­–ï¼š

- **ä½å»¶è¿Ÿ**: æ— éœ€å°†æ•°æ®ä¼ è¾“åˆ°äº‘ç«¯ï¼Œå®ç°å®æ—¶å“åº”
- **éšç§ä¿æŠ¤**: æ•æ„Ÿæ•°æ®æ— éœ€ç¦»å¼€è®¾å¤‡ï¼Œæé«˜å®‰å…¨æ€§
- **å¸¦å®½èŠ‚çœ**: å‡å°‘æ•°æ®ä¼ è¾“é‡ï¼Œé™ä½ç½‘ç»œè´Ÿè½½
- **ç¦»çº¿è¿è¡Œ**: åœ¨æ— ç½‘ç»œç¯å¢ƒä¸‹ä»ç„¶å¯ä»¥å·¥ä½œ

### 2.2 Pythonè¾¹ç¼˜AIæ¡†æ¶
**[æ ‡è¯†: EDGE-002]**

Pythonæä¾›äº†å¤šä¸ªé€‚ç”¨äºè¾¹ç¼˜è®¾å¤‡çš„è½»é‡çº§AIæ¡†æ¶ï¼š

#### TensorFlow Lite
TensorFlowçš„è½»é‡çº§ç‰ˆæœ¬ï¼Œä¸“ä¸ºç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¾å¤‡ä¼˜åŒ–ï¼š

```python
# TensorFlow Liteåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„åº”ç”¨ç¤ºä¾‹
import tensorflow as tf
import numpy as np

# 1. åˆ›å»ºæˆ–åŠ è½½æ¨¡å‹
# è¿™é‡Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç®€å•çš„åˆ†ç±»æ¨¡å‹
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])
model.compile(optimizer='adam', loss='categorical_crossentropy')

# 2. è½¬æ¢ä¸ºTFLiteæ¨¡å‹
converter = tf.lite.TFLiteConverter.from_keras_model(model)

# åº”ç”¨ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# é‡åŒ–ï¼ˆå¯é€‰ï¼‰
def representative_dataset_gen():
    for _ in range(100):
        yield [np.random.rand(1, 10).astype(np.float32)]

converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

# è½¬æ¢æ¨¡å‹
tflite_quant_model = converter.convert()

# 3. ä¿å­˜TFLiteæ¨¡å‹
with open('model.tflite', 'wb') as f:
    f.write(tflite_quant_model)

# 4. åŠ è½½å¹¶ä½¿ç”¨TFLiteæ¨¡å‹
interpreter = tf.lite.Interpreter(model_path='model.tflite')
interpreter.allocate_tensors()

# è·å–è¾“å…¥å’Œè¾“å‡ºå¼ é‡
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# å‡†å¤‡è¾“å…¥æ•°æ®
input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)

# è®¾ç½®è¾“å…¥æ•°æ®
interpreter.set_tensor(input_details[0]['index'], input_data)

# è¿è¡Œæ¨ç†
interpreter.invoke()

# è·å–è¾“å‡ºç»“æœ
output_data = interpreter.get_tensor(output_details[0]['index'])
print("æ¨ç†ç»“æœ:", output_data)
```

#### PyTorch Mobile
PyTorchçš„ç§»åŠ¨ç‰ˆæœ¬ï¼Œæ”¯æŒåœ¨ç§»åŠ¨è®¾å¤‡å’Œè¾¹ç¼˜è®¾å¤‡ä¸Šè¿è¡Œæ¨¡å‹ï¼š

```python
# PyTorch Mobileåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„åº”ç”¨ç¤ºä¾‹
import torch
import torch.nn as nn
import torch.nn.functional as F

# 1. å®šä¹‰ä¸€ä¸ªç®€å•çš„æ¨¡å‹
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(10, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 3)
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.softmax(self.fc3(x), dim=1)
        return x

# åˆ›å»ºæ¨¡å‹å®ä¾‹
model = SimpleModel()

# 2. å¯¼å‡ºä¸ºTorchScriptæ¨¡å‹
# è·Ÿè¸ªæ¨¡å‹
example_input = torch.randn(1, 10)
traced_model = torch.jit.trace(model, example_input)

# ä¿å­˜æ¨¡å‹
traced_model.save("model.pt")

# 3. é‡åŒ–æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
# åŠ¨æ€é‡åŒ–
quantized_model = torch.quantization.quantize_dynamic(
    model, {nn.Linear}, dtype=torch.qint8
)

# ä¿å­˜é‡åŒ–æ¨¡å‹
script_quantized_model = torch.jit.script(quantized_model)
script_quantized_model.save("quantized_model.pt")

# 4. åœ¨è¾¹ç¼˜è®¾å¤‡ä¸ŠåŠ è½½å¹¶ä½¿ç”¨æ¨¡å‹
# åœ¨å®é™…çš„è¾¹ç¼˜è®¾å¤‡ä¸Šï¼Œä»£ç ä¼šç±»ä¼¼è¿™æ ·ï¼š
loaded_model = torch.jit.load("model.pt")
loaded_model.eval()

# å‡†å¤‡è¾“å…¥
input_tensor = torch.randn(1, 10)

# è¿è¡Œæ¨ç†
with torch.no_grad():
    output = loaded_model(input_tensor)
    print("æ¨ç†ç»“æœ:", output)
```

#### ONNX Runtime
å¼€æ”¾ç¥ç»ç½‘ç»œäº¤æ¢æ ¼å¼è¿è¡Œæ—¶ï¼Œæ”¯æŒè·¨å¹³å°æ¨¡å‹éƒ¨ç½²ï¼š

```python
# ONNX Runtimeåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„åº”ç”¨ç¤ºä¾‹
import numpy as np
import onnx
import onnxruntime as ort

# å‡è®¾æˆ‘ä»¬å·²ç»æœ‰äº†ä¸€ä¸ªONNXæ¨¡å‹ï¼ˆmodel.onnxï¼‰

# åŠ è½½æ¨¡å‹
ort_session = ort.InferenceSession("model.onnx")

# è·å–è¾“å…¥å’Œè¾“å‡ºåç§°
input_name = ort_session.get_inputs()[0].name
output_name = ort_session.get_outputs()[0].name

# å‡†å¤‡è¾“å…¥æ•°æ®
input_data = np.random.randn(1, 10).astype(np.float32)

# è¿è¡Œæ¨ç†
outputs = ort_session.run([output_name], {input_name: input_data})

# è·å–ç»“æœ
result = outputs[0]
print("æ¨ç†ç»“æœ:", result)
```

### 2.3 ç‰©è”ç½‘è®¾å¤‡ä¸Pythoné›†æˆ
**[æ ‡è¯†: EDGE-003]**

Pythonå¯ä»¥è½»æ¾ä¸å„ç§ç‰©è”ç½‘è®¾å¤‡é›†æˆï¼Œå®ç°æ•°æ®é‡‡é›†å’Œæ§åˆ¶ï¼š

#### Raspberry Piä¸Šçš„Pythonåº”ç”¨
Raspberry Piæ˜¯æœ€å—æ¬¢è¿çš„ç‰©è”ç½‘å¼€å‘å¹³å°ä¹‹ä¸€ï¼š

```python
# Raspberry Piä¸Šçš„ä¼ æ„Ÿå™¨æ•°æ®é‡‡é›†ç¤ºä¾‹
import RPi.GPIO as GPIO
import time
import Adafruit_DHT
import requests

# è®¾ç½®GPIOæ¨¡å¼
GPIO.setmode(GPIO.BCM)

# å®šä¹‰ä¼ æ„Ÿå™¨å¼•è„š
DHT_SENSOR = Adafruit_DHT.DHT22
DHT_PIN = 4
LED_PIN = 17

# è®¾ç½®LEDå¼•è„šä¸ºè¾“å‡º
GPIO.setup(LED_PIN, GPIO.OUT)

def read_sensor_data():
    """è¯»å–æ¸©åº¦å’Œæ¹¿åº¦æ•°æ®"""
    humidity, temperature = Adafruit_DHT.read_retry(DHT_SENSOR, DHT_PIN)
    return humidity, temperature

def control_led(state):
    """æ§åˆ¶LEDç¯çš„å¼€å…³"""
    GPIO.output(LED_PIN, state)

def send_data_to_server(humidity, temperature):
    """å°†æ•°æ®å‘é€åˆ°æœåŠ¡å™¨"""
    url = "http://your-server.com/api/data"
    data = {
        "humidity": humidity,
        "temperature": temperature,
        "timestamp": time.time()
    }
    try:
        response = requests.post(url, json=data)
        print(f"æœåŠ¡å™¨å“åº”: {response.status_code}")
    except Exception as e:
        print(f"å‘é€æ•°æ®å¤±è´¥: {str(e)}")

def main():
    try:
        while True:
            humidity, temperature = read_sensor_data()
            
            if humidity is not None and temperature is not None:
                print(f"æ¸©åº¦: {temperature:.1f}Â°C, æ¹¿åº¦: {humidity:.1f}%")
                
                # æ ¹æ®æ¸©åº¦æ§åˆ¶LEDç¯
                if temperature > 25:
                    control_led(True)
                    print("æ¸©åº¦è¿‡é«˜ï¼ŒLEDç¯å·²å¼€å¯")
                else:
                    control_led(False)
                
                # å‘é€æ•°æ®åˆ°æœåŠ¡å™¨
                send_data_to_server(humidity, temperature)
            else:
                print("è¯»å–ä¼ æ„Ÿå™¨æ•°æ®å¤±è´¥")
            
            # æ¯10ç§’è¯»å–ä¸€æ¬¡æ•°æ®
            time.sleep(10)
    
    except KeyboardInterrupt:
        print("ç¨‹åºå·²åœæ­¢")
    finally:
        GPIO.cleanup()

if __name__ == "__main__":
    main()
```

#### MQTTåè®®ä¸Python
MQTTæ˜¯ç‰©è”ç½‘é¢†åŸŸå¹¿æ³›ä½¿ç”¨çš„è½»é‡çº§æ¶ˆæ¯ä¼ è¾“åè®®ï¼š

```python
# ä½¿ç”¨paho-mqttåº“è¿›è¡ŒMQTTé€šä¿¡
import paho.mqtt.client as mqtt
import time
import json

# MQTTä»£ç†è®¾ç½®
MQTT_BROKER = "broker.hivemq.com"  # ä½¿ç”¨å…¬å…±MQTTä»£ç†
MQTT_PORT = 1883
MQTT_TOPIC_TEMP = "sensors/temperature"
MQTT_TOPIC_HUM = "sensors/humidity"
MQTT_TOPIC_CONTROL = "devices/control"

# æ¨¡æ‹Ÿä¼ æ„Ÿå™¨æ•°æ®
def read_sensor_data():
    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šä»çœŸå®ä¼ æ„Ÿå™¨è¯»å–æ•°æ®
    import random
    temperature = random.uniform(20.0, 28.0)
    humidity = random.uniform(40.0, 70.0)
    return temperature, humidity

# å›è°ƒå‡½æ•°ï¼šè¿æ¥æˆåŠŸ
def on_connect(client, userdata, flags, rc):
    print(f"å·²è¿æ¥åˆ°MQTTä»£ç†ï¼Œè¿”å›ä»£ç : {rc}")
    # è®¢é˜…æ§åˆ¶ä¸»é¢˜
    client.subscribe(MQTT_TOPIC_CONTROL)

# å›è°ƒå‡½æ•°ï¼šæ”¶åˆ°æ¶ˆæ¯
def on_message(client, userdata, msg):
    print(f"æ”¶åˆ°æ¶ˆæ¯ä¸»é¢˜: {msg.topic}, å†…å®¹: {msg.payload.decode()}")
    # å¤„ç†æ§åˆ¶å‘½ä»¤
    try:
        control_data = json.loads(msg.payload.decode())
        if "led" in control_data:
            print(f"æ§åˆ¶LEDçŠ¶æ€: {control_data['led']}")
    except json.JSONDecodeError:
        print("æ— æ•ˆçš„JSONæ ¼å¼")

# åˆ›å»ºMQTTå®¢æˆ·ç«¯
client = mqtt.Client()

# è®¾ç½®å›è°ƒå‡½æ•°
client.on_connect = on_connect
client.on_message = on_message

# è¿æ¥åˆ°MQTTä»£ç†
client.connect(MQTT_BROKER, MQTT_PORT, 60)

# å¯åŠ¨å¾ªç¯ä»¥å¤„ç†ç½‘ç»œæµé‡
client.loop_start()

try:
    while True:
        # è¯»å–ä¼ æ„Ÿå™¨æ•°æ®
        temperature, humidity = read_sensor_data()
        
        # å‘å¸ƒæ¸©åº¦æ•°æ®
        client.publish(MQTT_TOPIC_TEMP, f"{temperature:.1f}")
        print(f"å·²å‘å¸ƒæ¸©åº¦: {temperature:.1f}Â°C")
        
        # å‘å¸ƒæ¹¿åº¦æ•°æ®
        client.publish(MQTT_TOPIC_HUM, f"{humidity:.1f}")
        print(f"å·²å‘å¸ƒæ¹¿åº¦: {humidity:.1f}%")
        
        # æ¯5ç§’å‘é€ä¸€æ¬¡æ•°æ®
        time.sleep(5)
        
except KeyboardInterrupt:
    print("ç¨‹åºå·²åœæ­¢")
finally:
    client.loop_stop()
    client.disconnect()
```

### 2.4 è¾¹ç¼˜AIåº”ç”¨æ¡ˆä¾‹
**[æ ‡è¯†: EDGE-004]**

Pythonåœ¨è¾¹ç¼˜AIé¢†åŸŸæœ‰ä¸°å¯Œçš„åº”ç”¨æ¡ˆä¾‹ï¼š

#### æ™ºèƒ½å®¶å±…ç³»ç»Ÿ
ç»“åˆè®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„æ™ºèƒ½å®¶å±…è§£å†³æ–¹æ¡ˆï¼š

```python
# ç®€åŒ–çš„æ™ºèƒ½å®¶å±…æ§åˆ¶ä¸­å¿ƒç¤ºä¾‹
import cv2
import numpy as np
import speech_recognition as sr
import pyttsx3
import threading
import time

class SmartHomeSystem:
    def __init__(self):
        # åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«å™¨å’Œæ–‡æœ¬è½¬è¯­éŸ³å¼•æ“
        self.recognizer = sr.Recognizer()
        self.engine = pyttsx3.init()
        self.engine.setProperty('rate', 150)  # è®¾ç½®è¯­é€Ÿ
        
        # è®¾å¤‡çŠ¶æ€
        self.devices = {
            "light": False,
            "air_conditioner": False,
            "tv": False
        }
        
        # å¯åŠ¨è§†é¢‘ç›‘æ§çº¿ç¨‹
        self.camera_thread = threading.Thread(target=self.monitor_camera)
        self.camera_thread.daemon = True
        
        # å¯åŠ¨è¯­éŸ³è¯†åˆ«çº¿ç¨‹
        self.voice_thread = threading.Thread(target=self.listen_for_commands)
        self.voice_thread.daemon = True
        
        self.running = False
    
    def speak(self, text):
        """è¯­éŸ³åˆæˆè¾“å‡º"""
        self.engine.say(text)
        self.engine.runAndWait()
    
    def process_command(self, command):
        """å¤„ç†è¯­éŸ³å‘½ä»¤"""
        command = command.lower()
        print(f"å¤„ç†å‘½ä»¤: {command}")
        
        # è®¾å¤‡æ§åˆ¶å‘½ä»¤
        if "æ‰“å¼€" in command:
            if "ç¯" in command or "ç¯å…‰" in command:
                self.devices["light"] = True
                self.speak("ç¯å·²æ‰“å¼€")
            elif "ç©ºè°ƒ" in command:
                self.devices["air_conditioner"] = True
                self.speak("ç©ºè°ƒå·²æ‰“å¼€")
            elif "ç”µè§†" in command:
                self.devices["tv"] = True
                self.speak("ç”µè§†å·²æ‰“å¼€")
        
        elif "å…³é—­" in command or "å…³" in command:
            if "ç¯" in command or "ç¯å…‰" in command:
                self.devices["light"] = False
                self.speak("ç¯å·²å…³é—­")
            elif "ç©ºè°ƒ" in command:
                self.devices["air_conditioner"] = False
                self.speak("ç©ºè°ƒå·²å…³é—­")
            elif "ç”µè§†" in command:
                self.devices["tv"] = False
                self.speak("ç”µè§†å·²å…³é—­")
        
        # æŸ¥è¯¢è®¾å¤‡çŠ¶æ€
        elif "çŠ¶æ€" in command or "æƒ…å†µ" in command:
            status_text = "å½“å‰çŠ¶æ€ï¼š"
            for device, state in self.devices.items():
                device_name = ""
                if device == "light":
                    device_name = "ç¯"
                elif device == "air_conditioner":
                    device_name = "ç©ºè°ƒ"
                elif device == "tv":
                    device_name = "ç”µè§†"
                status_text += f"{device_name}{'å¼€å¯' if state else 'å…³é—­'}ï¼Œ"
            self.speak(status_text[:-1])  # å»æ‰æœ€åçš„é€—å·
        
        else:
            self.speak("æŠ±æ­‰ï¼Œæˆ‘ä¸ç†è§£æ‚¨çš„å‘½ä»¤")
    
    def listen_for_commands(self):
        """ç›‘å¬è¯­éŸ³å‘½ä»¤"""
        while self.running:
            with sr.Microphone() as source:
                print("æ­£åœ¨ç›‘å¬...")
                self.recognizer.adjust_for_ambient_noise(source)
                try:
                    audio = self.recognizer.listen(source, timeout=5)
                    command = self.recognizer.recognize_google(audio, language='zh-CN')
                    print(f"è¯†åˆ«åˆ°å‘½ä»¤: {command}")
                    self.process_command(command)
                except sr.UnknownValueError:
                    print("æ— æ³•è¯†åˆ«è¯­éŸ³")
                except sr.RequestError as e:
                    print(f"è¯­éŸ³è¯†åˆ«æœåŠ¡é”™è¯¯: {e}")
                except sr.WaitTimeoutError:
                    pass  # è¶…æ—¶ï¼Œç»§ç»­ç›‘å¬
    
    def monitor_camera(self):
        """ç›‘æ§æ‘„åƒå¤´å¹¶è¿›è¡Œç®€å•çš„è¿åŠ¨æ£€æµ‹"""
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šä½¿ç”¨TensorFlow Liteæˆ–OpenCVè¿›è¡Œå¯¹è±¡æ£€æµ‹
        # ç®€åŒ–ç¤ºä¾‹ä¸­åªè¿›è¡ŒåŸºæœ¬çš„è¿åŠ¨æ£€æµ‹
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
            return
        
        # è·å–åˆå§‹å¸§
        ret, frame1 = cap.read()
        if not ret:
            print("æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
            cap.release()
            return
        
        gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
        gray1 = cv2.GaussianBlur(gray1, (21, 21), 0)
        
        while self.running:
            ret, frame2 = cap.read()
            if not ret:
                break
            
            gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)
            gray2 = cv2.GaussianBlur(gray2, (21, 21), 0)
            
            # è®¡ç®—å¸§å·®
            frame_diff = cv2.absdiff(gray1, gray2)
            thresh = cv2.threshold(frame_diff, 25, 255, cv2.THRESH_BINARY)[1]
            thresh = cv2.dilate(thresh, None, iterations=2)
            
            # æŸ¥æ‰¾è½®å»“
            contours, _ = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            # æ£€æµ‹æ˜¯å¦æœ‰è¿åŠ¨
            motion_detected = False
            for contour in contours:
                if cv2.contourArea(contour) > 1000:  # å¿½ç•¥å°çš„è½®å»“
                    motion_detected = True
                    break
            
            if motion_detected:
                print("æ£€æµ‹åˆ°è¿åŠ¨")
                # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè§¦å‘ç›¸åº”çš„åŠ¨ä½œï¼Œå¦‚å‘é€é€šçŸ¥ç­‰
            
            # æ›´æ–°å‚è€ƒå¸§
            gray1 = gray2.copy()
            
            # æ˜¾ç¤ºè§†é¢‘ï¼ˆå¯é€‰ï¼‰
            cv2.imshow("Smart Home Camera", frame2)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()
    
    def start(self):
        """å¯åŠ¨æ™ºèƒ½å®¶å±…ç³»ç»Ÿ"""
        self.running = True
        self.speak("æ™ºèƒ½å®¶å±…ç³»ç»Ÿå·²å¯åŠ¨")
        self.camera_thread.start()
        self.voice_thread.start()
        print("æ™ºèƒ½å®¶å±…ç³»ç»Ÿæ­£åœ¨è¿è¡Œ...")
    
    def stop(self):
        """åœæ­¢æ™ºèƒ½å®¶å±…ç³»ç»Ÿ"""
        self.running = False
        self.speak("æ™ºèƒ½å®¶å±…ç³»ç»Ÿå·²åœæ­¢")
        if self.camera_thread.is_alive():
            self.camera_thread.join(timeout=1.0)
        if self.voice_thread.is_alive():
            self.voice_thread.join(timeout=1.0)
        print("æ™ºèƒ½å®¶å±…ç³»ç»Ÿå·²åœæ­¢")

# è¿è¡Œæ™ºèƒ½å®¶å±…ç³»ç»Ÿ
try:
    smart_home = SmartHomeSystem()
    smart_home.start()
    while True:
        time.sleep(1)  # ä¿æŒä¸»ç¨‹åºè¿è¡Œ
except KeyboardInterrupt:
    print("\nç¨‹åºå·²åœæ­¢")
    smart_home.stop()
```

#### å·¥ä¸šç‰©è”ç½‘é¢„æµ‹æ€§ç»´æŠ¤
ä½¿ç”¨æœºå™¨å­¦ä¹ ç®—æ³•é¢„æµ‹è®¾å¤‡æ•…éšœï¼Œå®ç°é¢„æµ‹æ€§ç»´æŠ¤ï¼š

```python
# å·¥ä¸šè®¾å¤‡é¢„æµ‹æ€§ç»´æŠ¤ç¤ºä¾‹
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import joblib

# æ¨¡æ‹Ÿè®¾å¤‡ä¼ æ„Ÿå™¨æ•°æ®ç”Ÿæˆ
def generate_sensor_data(n_samples=1000):
    """ç”Ÿæˆæ¨¡æ‹Ÿçš„è®¾å¤‡ä¼ æ„Ÿå™¨æ•°æ®"""
    np.random.seed(42)
    
    # æ­£å¸¸è¿è¡Œæ—¶çš„å‚æ•°èŒƒå›´
    temp_normal = np.random.normal(75, 5, n_samples)  # æ¸©åº¦
    vibration_normal = np.random.normal(0.5, 0.1, n_samples)  # æŒ¯åŠ¨
    pressure_normal = np.random.normal(100, 5, n_samples)  # å‹åŠ›
    current_normal = np.random.normal(10, 1, n_samples)  # ç”µæµ
    
    # åˆ›å»ºæ•…éšœæ•°æ®ï¼ˆé€šè¿‡ä¿®æ”¹å‚æ•°å€¼ï¼‰
    n_faulty = int(n_samples * 0.2)  # 20%çš„æ•°æ®ä¸ºæ•…éšœæ•°æ®
    
    # é€‰æ‹©n_faultyä¸ªæ ·æœ¬æ ‡è®°ä¸ºæ•…éšœ
    fault_indices = np.random.choice(n_samples, n_faulty, replace=False)
    
    # ä¿®æ”¹æ•…éšœæ ·æœ¬çš„å‚æ•°å€¼
    temp = temp_normal.copy()
    vibration = vibration_normal.copy()
    pressure = pressure_normal.copy()
    current = current_normal.copy()
    
    # æ•…éšœæ¨¡å¼1: æ¸©åº¦é«˜ï¼ŒæŒ¯åŠ¨å¤§
    fault1_indices = fault_indices[:int(n_faulty * 0.4)]
    temp[fault1_indices] = np.random.normal(95, 8, len(fault1_indices))
    vibration[fault1_indices] = np.random.normal(1.5, 0.3, len(fault1_indices))
    
    # æ•…éšœæ¨¡å¼2: å‹åŠ›ä½ï¼Œç”µæµä¸ç¨³å®š
    fault2_indices = fault_indices[int(n_faulty * 0.4):]
    pressure[fault2_indices] = np.random.normal(70, 5, len(fault2_indices))
    current[fault2_indices] = np.random.normal(15, 3, len(fault2_indices))
    
    # åˆ›å»ºæ ‡ç­¾ï¼š0=æ­£å¸¸ï¼Œ1=æ•…éšœ
    labels = np.zeros(n_samples)
    labels[fault_indices] = 1
    
    # åˆ›å»ºæ—¶é—´åºåˆ—
    timestamps = pd.date_range(start='2023-01-01', periods=n_samples, freq='H')
    
    # åˆ›å»ºDataFrame
    data = pd.DataFrame({
        'timestamp': timestamps,
        'temperature': temp,
        'vibration': vibration,
        'pressure': pressure,
        'current': current,
        'fault': labels
    })
    
    return data

# è®­ç»ƒæ¨¡å‹
def train_predictive_maintenance_model():
    """è®­ç»ƒé¢„æµ‹æ€§ç»´æŠ¤æ¨¡å‹"""
    # ç”Ÿæˆæ•°æ®
    data = generate_sensor_data()
    
    # ç‰¹å¾å·¥ç¨‹ï¼šæ·»åŠ æ—¶é—´ç›¸å…³ç‰¹å¾
    data['hour'] = data['timestamp'].dt.hour
    data['day_of_week'] = data['timestamp'].dt.dayofweek
    
    # ç‰¹å¾å·¥ç¨‹ï¼šæ·»åŠ ç»Ÿè®¡ç‰¹å¾ï¼ˆæ»‘åŠ¨çª—å£ï¼‰
    window_size = 24  # 24å°æ—¶çª—å£
    for sensor in ['temperature', 'vibration', 'pressure', 'current']:
        data[f'{sensor}_mean'] = data[sensor].rolling(window=window_size).mean()
        data[f'{sensor}_std'] = data[sensor].rolling(window=window_size).std()
        data[f'{sensor}_min'] = data[sensor].rolling(window=window_size).min()
        data[f'{sensor}_max'] = data[sensor].rolling(window=window_size).max()
    
    # å»æ‰å«æœ‰NaNçš„è¡Œ
    data.dropna(inplace=True)
    
    # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
    features = [col for col in data.columns if col not in ['timestamp', 'fault']]
    X = data[features]
    y = data['fault']
    
    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train_scaled, y_train)
    
    # è¯„ä¼°æ¨¡å‹
    y_pred = model.predict(X_test_scaled)
    print("åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred))
    
    print("æ··æ·†çŸ©é˜µ:")
    print(confusion_matrix(y_test, y_pred))
    
    # ç‰¹å¾é‡è¦æ€§
    feature_importances = pd.DataFrame({
        'feature': features,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("ç‰¹å¾é‡è¦æ€§:")
    print(feature_importances)
    
    # ä¿å­˜æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨
    joblib.dump(model, 'predictive_maintenance_model.pkl')
    joblib.dump(scaler, 'scaler.pkl')
    
    return model, scaler, data

# å®æ—¶ç›‘æ§å’Œé¢„æµ‹
def monitor_device(model, scaler):
    """æ¨¡æ‹Ÿå®æ—¶è®¾å¤‡ç›‘æ§å’Œæ•…éšœé¢„æµ‹"""
    print("å¼€å§‹è®¾å¤‡ç›‘æ§...")
    
    # æ¨¡æ‹Ÿå®æ—¶æ•°æ®æµ
    n_samples = 100
    history = []
    
    for i in range(n_samples):
        # æ¨¡æ‹Ÿä¼ æ„Ÿå™¨æ•°æ®ï¼ˆå®é™…åº”ç”¨ä¸­ä¼šä»è®¾å¤‡è¯»å–ï¼‰
        temp = np.random.normal(75, 5)
        vibration = np.random.normal(0.5, 0.1)
        pressure = np.random.normal(100, 5)
        current = np.random.normal(10, 1)
        
        # éšæœºæ¨¡æ‹Ÿæ•…éšœ
        if np.random.random() < 0.1:  # 10%æ¦‚ç‡æ¨¡æ‹Ÿæ•…éšœ
            temp = np.random.normal(90, 10)
            vibration = np.random.normal(1.2, 0.4)
        
        # åˆ›å»ºæ•°æ®ç‚¹
        data_point = {
            'temperature': temp,
            'vibration': vibration,
            'pressure': pressure,
            'current': current,
            'hour': pd.Timestamp.now().hour,
            'day_of_week': pd.Timestamp.now().dayofweek
        }
        
        # æ·»åŠ åˆ°å†å²è®°å½•
        history.append(data_point)
        
        # å½“æœ‰è¶³å¤Ÿå†å²æ•°æ®æ—¶è¿›è¡Œé¢„æµ‹
        if len(history) >= 24:
            # è®¡ç®—ç»Ÿè®¡ç‰¹å¾
            df_history = pd.DataFrame(history[-24:])
            for sensor in ['temperature', 'vibration', 'pressure', 'current']:
                data_point[f'{sensor}_mean'] = df_history[sensor].mean()
                data_point[f'{sensor}_std'] = df_history[sensor].std()
                data_point[f'{sensor}_min'] = df_history[sensor].min()
                data_point[f'{sensor}_max'] = df_history[sensor].max()
            
            # å‡†å¤‡ç‰¹å¾å‘é‡
            features = [col for col in data_point.keys()]
            X = pd.DataFrame([data_point])[features]
            X_scaled = scaler.transform(X)
            
            # é¢„æµ‹
            prediction = model.predict(X_scaled)[0]
            probability = model.predict_proba(X_scaled)[0][1]
            
            # è¾“å‡ºç»“æœ
            if prediction == 1:
                print(f"âš ï¸  è­¦å‘Š: é¢„æµ‹è®¾å¤‡å°†å‘ç”Ÿæ•…éšœ! æ¦‚ç‡: {probability:.2f}")
            else:
                print(f"âœ… æ­£å¸¸: è®¾å¤‡è¿è¡Œæ­£å¸¸. æ•…éšœæ¦‚ç‡: {probability:.2f}")
        else:
            print(f"ğŸ“Š æ”¶é›†æ•°æ®ä¸­: {len(history)}/24")
        
        # æ¨¡æ‹Ÿå®æ—¶æ•°æ®æµé—´éš”
        import time
        time.sleep(0.5)

# è¿è¡Œç¤ºä¾‹
def main():
    # è®­ç»ƒæ¨¡å‹
    print("è®­ç»ƒé¢„æµ‹æ€§ç»´æŠ¤æ¨¡å‹...")
    model, scaler, _ = train_predictive_maintenance_model()
    
    # ç›‘æ§è®¾å¤‡
    try:
        monitor_device(model, scaler)
    except KeyboardInterrupt:
        print("ç›‘æ§å·²åœæ­¢")

if __name__ == "__main__":
    main()

## 3. ä½ä»£ç ä¸æ— ä»£ç å¼€å‘

### 3.1 ä½ä»£ç å¼€å‘å¹³å°
**[æ ‡è¯†: LOWCODE-001]**

ä½ä»£ç å¼€å‘å¹³å°é€šè¿‡å¯è§†åŒ–ç•Œé¢å’Œé…ç½®åŒ–æ–¹å¼ï¼Œå¤§å¹…å‡å°‘ä¼ ç»Ÿç¼–ç¨‹å·¥ä½œé‡ï¼š

- **å¯è§†åŒ–å¼€å‘**ï¼šé€šè¿‡æ‹–æ‹½ç»„ä»¶å’Œé…ç½®å±æ€§æ„å»ºåº”ç”¨
- **å¿«é€ŸåŸå‹**ï¼šåŠ é€Ÿåº”ç”¨å¼€å‘å‘¨æœŸï¼Œå®ç°å¿«é€Ÿè¿­ä»£
- **é™ä½é—¨æ§›**ï¼šéä¸“ä¸šå¼€å‘è€…ä¹Ÿèƒ½å‚ä¸åº”ç”¨å¼€å‘
- **ä¼ä¸šçº§èƒ½åŠ›**ï¼šæä¾›å®‰å…¨æ€§ã€å¯æ‰©å±•æ€§å’Œé›†æˆèƒ½åŠ›

### 3.2 Pythonä½ä»£ç æ¡†æ¶
**[æ ‡è¯†: LOWCODE-002]**

Pythonç”Ÿæ€ç³»ç»Ÿä¸­æœ‰å¤šä¸ªæˆç†Ÿçš„ä½ä»£ç /æ— ä»£ç æ¡†æ¶ï¼š

#### Streamlit
ä¸“ä¸ºæ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ åº”ç”¨æ‰“é€ çš„å¿«é€Ÿå¼€å‘æ¡†æ¶ï¼š

```python
# ä½¿ç”¨Streamlitåˆ›å»ºæ•°æ®å¯è§†åŒ–åº”ç”¨
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®é¡µé¢æ ‡é¢˜
st.title('æ•°æ®åˆ†æä»ªè¡¨æ¿')

# æ·»åŠ ä¾§è¾¹æ 
st.sidebar.header('å‚æ•°è®¾ç½®')

# æ–‡ä»¶ä¸Šä¼ å™¨
uploaded_file = st.sidebar.file_uploader("ä¸Šä¼ CSVæ–‡ä»¶", type=["csv"])

if uploaded_file is not None:
    # è¯»å–æ•°æ®
    df = pd.read_csv(uploaded_file)
    
    # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
    st.subheader('æ•°æ®é¢„è§ˆ')
    st.dataframe(df.head())
    
    # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡ä¿¡æ¯
    st.subheader('æ•°æ®ç»Ÿè®¡')
    st.write(df.describe())
    
    # é€‰æ‹©è¦åˆ†æçš„åˆ—
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    x_axis = st.sidebar.selectbox('Xè½´', numeric_cols)
    y_axis = st.sidebar.selectbox('Yè½´', numeric_cols)
    
    # é€‰æ‹©å›¾è¡¨ç±»å‹
    chart_type = st.sidebar.radio('å›¾è¡¨ç±»å‹', ['æ•£ç‚¹å›¾', 'æŠ˜çº¿å›¾', 'æŸ±çŠ¶å›¾', 'çƒ­åŠ›å›¾'])
    
    # ç»˜åˆ¶å›¾è¡¨
    st.subheader(f'{chart_type}')
    
    if chart_type == 'æ•£ç‚¹å›¾':
        fig, ax = plt.subplots()
        ax.scatter(df[x_axis], df[y_axis])
        ax.set_xlabel(x_axis)
        ax.set_ylabel(y_axis)
        st.pyplot(fig)
    
    elif chart_type == 'æŠ˜çº¿å›¾':
        fig, ax = plt.subplots()
        ax.plot(df[x_axis], df[y_axis])
        ax.set_xlabel(x_axis)
        ax.set_ylabel(y_axis)
        st.pyplot(fig)
    
    elif chart_type == 'æŸ±çŠ¶å›¾':
        fig, ax = plt.subplots()
        df.groupby(x_axis).mean()[y_axis].plot(kind='bar', ax=ax)
        ax.set_xlabel(x_axis)
        ax.set_ylabel(y_axis)
        st.pyplot(fig)
    
    elif chart_type == 'çƒ­åŠ›å›¾':
        fig, ax = plt.subplots(figsize=(10, 8))
        corr = df.select_dtypes(include=[np.number]).corr()
        sns.heatmap(corr, annot=True, cmap='coolwarm', ax=ax)
        st.pyplot(fig)
    
    # æ·»åŠ ç®€å•çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚æœæœ‰ç›®æ ‡å˜é‡ï¼‰
    if st.sidebar.checkbox('è¿è¡Œç®€å•é¢„æµ‹æ¨¡å‹'):
        target = st.sidebar.selectbox('é€‰æ‹©ç›®æ ‡å˜é‡', numeric_cols)
        
        # ç®€å•çº¿æ€§å›å½’
        from sklearn.model_selection import train_test_split
        from sklearn.linear_model import LinearRegression
        from sklearn.metrics import r2_score, mean_squared_error
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
        features = [col for col in numeric_cols if col != target]
        X = df[features]
        y = df[target]
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # è®­ç»ƒæ¨¡å‹
        model = LinearRegression()
        model.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred = model.predict(X_test)
        
        # è¯„ä¼°
        r2 = r2_score(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        
        # æ˜¾ç¤ºç»“æœ
        st.subheader('é¢„æµ‹æ¨¡å‹ç»“æœ')
        st.write(f'RÂ²å¾—åˆ†: {r2:.4f}')
        st.write(f'å‡æ–¹è¯¯å·®: {mse:.4f}')
        
        # ç»˜åˆ¶é¢„æµ‹vså®é™…å€¼
        fig, ax = plt.subplots()
        ax.scatter(y_test, y_pred)
        ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
        ax.set_xlabel('å®é™…å€¼')
        ax.set_ylabel('é¢„æµ‹å€¼')
        st.pyplot(fig)
else:
    st.info('è¯·ä¸Šä¼ CSVæ–‡ä»¶ä»¥å¼€å§‹åˆ†æ')
```

#### Gradio
åˆ›å»ºæœºå™¨å­¦ä¹ æ¨¡å‹æ¼”ç¤ºç•Œé¢çš„ç®€å•æ¡†æ¶ï¼š

```python
# ä½¿ç”¨Gradioåˆ›å»ºå›¾åƒåˆ†ç±»æ¼”ç¤º
import gradio as gr
import numpy as np
import tensorflow as tf
from PIL import Image

# åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹ï¼ˆè¿™é‡Œä½¿ç”¨TensorFlowçš„MobileNetV2ä½œä¸ºç¤ºä¾‹ï¼‰
def load_model():
    model = tf.keras.applications.MobileNetV2(weights='imagenet')
    return model

# åŠ è½½ImageNetç±»åˆ«æ ‡ç­¾
def load_labels():
    labels_path = tf.keras.utils.get_file(
        'ImageNetLabels.txt',
        'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')
    imagenet_labels = np.array(open(labels_path).read().splitlines())
    return imagenet_labels

# é¢„å¤„ç†å›¾åƒ
def preprocess_image(img):
    img = img.resize((224, 224))
    img_array = tf.keras.preprocessing.image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)
    return img_array

# é¢„æµ‹å‡½æ•°
def predict_image(img):
    model = load_model()
    labels = load_labels()
    
    # é¢„å¤„ç†å›¾åƒ
    processed_img = preprocess_image(img)
    
    # é¢„æµ‹
    predictions = model.predict(processed_img)
    decoded_predictions = tf.keras.applications.mobilenet_v2.decode_predictions(predictions, top=5)[0]
    
    # æ ¼å¼åŒ–ç»“æœ
    result = {label: float(score) for (_, label, score) in decoded_predictions}
    
    return result

# åˆ›å»ºGradioç•Œé¢
interface = gr.Interface(
    fn=predict_image,
    inputs=gr.Image(type="pil"),
    outputs=gr.Label(num_top_classes=5),
    title="å›¾åƒåˆ†ç±»æ¼”ç¤º",
    description="ä¸Šä¼ ä¸€å¼ å›¾åƒï¼Œæ¨¡å‹å°†é¢„æµ‹å›¾åƒä¸­åŒ…å«çš„ç‰©ä½“ã€‚",
    examples=[
        ["cat.jpg"],
        ["dog.jpg"],
        ["bird.jpg"]
    ]
)

# å¯åŠ¨ç•Œé¢
if __name__ == "__main__":
    interface.launch()
```

#### Plotly Dash
æ„å»ºäº¤äº’å¼Webåº”ç”¨çš„ä¼ä¸šçº§æ¡†æ¶ï¼š

```python
# ä½¿ç”¨Plotly Dashåˆ›å»ºäº¤äº’å¼ä»ªè¡¨æ¿
import dash
from dash import dcc, html
from dash.dependencies import Input, Output
import plotly.express as px
import pandas as pd
import numpy as np

# åˆ›å»ºç¤ºä¾‹æ•°æ®
def generate_data():
    np.random.seed(42)
    dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')
    regions = ['ååŒ—', 'åä¸œ', 'åå—', 'è¥¿å—', 'ä¸œåŒ—']
    products = ['äº§å“A', 'äº§å“B', 'äº§å“C', 'äº§å“D']
    
    data = []
    for date in dates:
        for region in regions:
            for product in products:
                # ç”Ÿæˆä¸€äº›æœ‰å­£èŠ‚æ€§å’Œè¶‹åŠ¿çš„æ•°æ®
                base_sales = 100 + np.random.randint(-20, 20)
                seasonal = 50 * np.sin(2 * np.pi * (date.dayofyear / 365))
                trend = 0.1 * date.dayofyear
                region_factor = {'ååŒ—': 1.2, 'åä¸œ': 1.5, 'åå—': 1.3, 'è¥¿å—': 0.9, 'ä¸œåŒ—': 0.8}[region]
                product_factor = {'äº§å“A': 1.5, 'äº§å“B': 1.2, 'äº§å“C': 0.9, 'äº§å“D': 1.1}[product]
                
                sales = base_sales + seasonal + trend * region_factor * product_factor
                profit = sales * (0.2 + 0.1 * np.random.random())
                
                data.append({
                    'æ—¥æœŸ': date,
                    'åœ°åŒº': region,
                    'äº§å“': product,
                    'é”€å”®é¢': max(0, sales),
                    'åˆ©æ¶¦': max(0, profit)
                })
    
    return pd.DataFrame(data)

# åˆå§‹åŒ–Dashåº”ç”¨
app = dash.Dash(__name__, title='é”€å”®æ•°æ®ä»ªè¡¨æ¿')

# ç”Ÿæˆæ•°æ®
df = generate_data()

# å¸ƒå±€
app.layout = html.Div([
    html.H1("é”€å”®æ•°æ®äº¤äº’å¼ä»ªè¡¨æ¿"),
    
    html.Div([
        html.Div([
            html.Label("é€‰æ‹©åœ°åŒº:"),
            dcc.Dropdown(
                id='region-dropdown',
                options=[{'label': region, 'value': region} for region in df['åœ°åŒº'].unique()],
                value=df['åœ°åŒº'].unique(),
                multi=True
            )
        ], style={'width': '48%', 'display': 'inline-block'}),
        
        html.Div([
            html.Label("é€‰æ‹©äº§å“:"),
            dcc.Dropdown(
                id='product-dropdown',
                options=[{'label': product, 'value': product} for product in df['äº§å“'].unique()],
                value=df['äº§å“'].unique(),
                multi=True
            )
        ], style={'width': '48%', 'display': 'inline-block', 'float': 'right'})
    ]),
    
    html.Div([
        html.Div([
            html.H3("é”€å”®é¢è¶‹åŠ¿"),
            dcc.Graph(id='sales-trend-graph')
        ], style={'width': '48%', 'display': 'inline-block'}),
        
        html.Div([
            html.H3("åˆ©æ¶¦åˆ†æ"),
            dcc.Graph(id='profit-graph')
        ], style={'width': '48%', 'display': 'inline-block', 'float': 'right'})
    ]),
    
    html.Div([
        html.H3("åœ°åŒºä¸äº§å“é”€å”®é¢çƒ­åŠ›å›¾"),
        dcc.Graph(id='heatmap')
    ])
])

# å›è°ƒå‡½æ•°ï¼šæ›´æ–°é”€å”®é¢è¶‹åŠ¿å›¾
@app.callback(
    Output('sales-trend-graph', 'figure'),
    [Input('region-dropdown', 'value'),
     Input('product-dropdown', 'value')]
)
def update_sales_trend(selected_regions, selected_products):
    # ç­›é€‰æ•°æ®
    filtered_df = df[(df['åœ°åŒº'].isin(selected_regions)) & 
                     (df['äº§å“'].isin(selected_products))]
    
    # æŒ‰æ—¥æœŸå’Œäº§å“åˆ†ç»„ï¼Œè®¡ç®—é”€å”®é¢
    grouped_df = filtered_df.groupby(['æ—¥æœŸ', 'äº§å“'])['é”€å”®é¢'].sum().reset_index()
    
    # åˆ›å»ºå›¾è¡¨
    fig = px.line(grouped_df, x='æ—¥æœŸ', y='é”€å”®é¢', color='äº§å“',
                  title='é”€å”®é¢è¶‹åŠ¿')
    
    return fig

# å›è°ƒå‡½æ•°ï¼šæ›´æ–°åˆ©æ¶¦åˆ†æå›¾
@app.callback(
    Output('profit-graph', 'figure'),
    [Input('region-dropdown', 'value'),
     Input('product-dropdown', 'value')]
)
def update_profit_graph(selected_regions, selected_products):
    # ç­›é€‰æ•°æ®
    filtered_df = df[(df['åœ°åŒº'].isin(selected_regions)) & 
                     (df['äº§å“'].isin(selected_products))]
    
    # æŒ‰åœ°åŒºå’Œäº§å“åˆ†ç»„ï¼Œè®¡ç®—å¹³å‡åˆ©æ¶¦
    grouped_df = filtered_df.groupby(['åœ°åŒº', 'äº§å“'])['åˆ©æ¶¦'].mean().reset_index()
    
    # åˆ›å»ºå›¾è¡¨
    fig = px.bar(grouped_df, x='åœ°åŒº', y='åˆ©æ¶¦', color='äº§å“',
                 barmode='group', title='å„åœ°åŒºäº§å“å¹³å‡åˆ©æ¶¦')
    
    return fig

# å›è°ƒå‡½æ•°ï¼šæ›´æ–°çƒ­åŠ›å›¾
@app.callback(
    Output('heatmap', 'figure'),
    [Input('region-dropdown', 'value'),
     Input('product-dropdown', 'value')]
)
def update_heatmap(selected_regions, selected_products):
    # ç­›é€‰æ•°æ®
    filtered_df = df[(df['åœ°åŒº'].isin(selected_regions)) & 
                     (df['äº§å“'].isin(selected_products))]
    
    # æŒ‰åœ°åŒºå’Œäº§å“åˆ†ç»„ï¼Œè®¡ç®—æ€»é”€å”®é¢
    pivot_df = filtered_df.pivot_table(values='é”€å”®é¢', 
                                      index='åœ°åŒº', 
                                      columns='äº§å“', 
                                      aggfunc='sum')
    
    # åˆ›å»ºçƒ­åŠ›å›¾
    fig = px.imshow(pivot_df, 
                    labels=dict(x='äº§å“', y='åœ°åŒº', color='æ€»é”€å”®é¢'),
                    x=pivot_df.columns,
                    y=pivot_df.index,
                    title='åœ°åŒºä¸äº§å“é”€å”®é¢çƒ­åŠ›å›¾')
    
    return fig

# è¿è¡Œåº”ç”¨
if __name__ == '__main__':
    app.run_server(debug=True)
```

### 3.3 ä½ä»£ç ä¸Pythonç»“åˆçš„ä¼˜åŠ¿
**[æ ‡è¯†: LOWCODE-003]**

Pythonä¸ä½ä»£ç å¼€å‘ç»“åˆå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼š

- **çµæ´»æ‰©å±•**ï¼šåœ¨å¯è§†åŒ–ç•Œé¢ä¹‹å¤–ï¼Œå¯é€šè¿‡Pythonä»£ç å®ç°å¤æ‚é€»è¾‘
- **æ•°æ®ç§‘å­¦èƒ½åŠ›**ï¼šæ— ç¼é›†æˆPythonå¼ºå¤§çš„æ•°æ®å¤„ç†å’Œåˆ†æåº“
- **æœºå™¨å­¦ä¹ é›†æˆ**ï¼šè½»æ¾éƒ¨ç½²å’Œä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹
- **å…¨æ ˆå¼€å‘**ï¼šä»å‰ç«¯ç•Œé¢åˆ°åç«¯é€»è¾‘çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ
- **æˆç†Ÿç”Ÿæ€**ï¼šä¸°å¯Œçš„ç¬¬ä¸‰æ–¹åº“æ”¯æŒå„ç§ä¸“ä¸šé¢†åŸŸçš„éœ€æ±‚

### 3.4 ä½ä»£ç å¼€å‘æœ€ä½³å®è·µ
**[æ ‡è¯†: LOWCODE-004]**

æˆåŠŸå®æ–½ä½ä»£ç å¼€å‘çš„å…³é”®å®è·µï¼š

```python
# ä½ä»£ç åº”ç”¨æ¶æ„ç¤ºä¾‹
from fastapi import FastAPI, Depends, HTTPException
from sqlalchemy import create_engine, Column, Integer, String, Float
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
from pydantic import BaseModel
import pandas as pd
import plotly.express as px
import plotly.io as pio

# 1. æ•°æ®å±‚ - æ•°æ®åº“æ¨¡å‹
SQLALCHEMY_DATABASE_URL = "sqlite:///./data.db"

engine = create_engine(
    SQLALCHEMY_DATABASE_URL, connect_args={"check_same_thread": False}
)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()

class Product(Base):
    __tablename__ = "products"
    
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, index=True)
    category = Column(String, index=True)
    price = Column(Float)
    stock = Column(Integer)

# åˆ›å»ºæ•°æ®åº“è¡¨
Base.metadata.create_all(bind=engine)

# 2. ä¸šåŠ¡é€»è¾‘å±‚ - CRUDæ“ä½œ
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# Pydanticæ¨¡å‹ç”¨äºæ•°æ®éªŒè¯å’Œåºåˆ—åŒ–
class ProductBase(BaseModel):
    name: str
    category: str
    price: float
    stock: int

class ProductCreate(ProductBase):
    pass

class ProductResponse(ProductBase):
    id: int
    
    class Config:
        orm_mode = True

# 3. APIå±‚ - FastAPIæ¥å£
app = FastAPI()

@app.post("/products/", response_model=ProductResponse)
def create_product(product: ProductCreate, db: Session = Depends(get_db)):
    db_product = Product(**product.dict())
    db.add(db_product)
    db.commit()
    db.refresh(db_product)
    return db_product

@app.get("/products/", response_model=list[ProductResponse])
def read_products(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)):
    products = db.query(Product).offset(skip).limit(limit).all()
    return products

@app.get("/products/{product_id}", response_model=ProductResponse)
def read_product(product_id: int, db: Session = Depends(get_db)):
    db_product = db.query(Product).filter(Product.id == product_id).first()
    if db_product is None:
        raise HTTPException(status_code=404, detail="äº§å“ä¸å­˜åœ¨")
    return db_product

@app.put("/products/{product_id}", response_model=ProductResponse)
def update_product(product_id: int, product: ProductCreate, db: Session = Depends(get_db)):
    db_product = db.query(Product).filter(Product.id == product_id).first()
    if db_product is None:
        raise HTTPException(status_code=404, detail="äº§å“ä¸å­˜åœ¨")
    
    for key, value in product.dict().items():
        setattr(db_product, key, value)
    
    db.commit()
    db.refresh(db_product)
    return db_product

@app.delete("/products/{product_id}")
def delete_product(product_id: int, db: Session = Depends(get_db)):
    db_product = db.query(Product).filter(Product.id == product_id).first()
    if db_product is None:
        raise HTTPException(status_code=404, detail="äº§å“ä¸å­˜åœ¨")
    
    db.delete(db_product)
    db.commit()
    return {"detail": "äº§å“å·²åˆ é™¤"}

# 4. æ•°æ®åˆ†æå’Œå¯è§†åŒ–API
@app.get("/analytics/sales-by-category")
def sales_by_category(db: Session = Depends(get_db)):
    # å‡è®¾è¿™é‡Œæœ‰é”€å”®æ•°æ®ï¼Œç®€åŒ–ç¤ºä¾‹
    products = db.query(Product).all()
    df = pd.DataFrame([
        {"category": p.category, "price": p.price, "stock": p.stock}
        for p in products
    ])
    
    # æŒ‰ç±»åˆ«èšåˆ
    if not df.empty:
        category_summary = df.groupby('category').agg({
            'price': 'sum',
            'stock': 'sum'
        }).reset_index()
        
        # åˆ›å»ºå¯è§†åŒ–
        fig = px.bar(category_summary, x='category', y='price', 
                     title='æŒ‰ç±»åˆ«é”€å”®é¢')
        
        # è¿”å›å›¾è¡¨JSON
        return {"data": category_summary.to_dict('records'), "chart": fig.to_json()}
    else:
        return {"data": [], "chart": None}

# å¯åŠ¨åº”ç”¨ç¤ºä¾‹ï¼ˆå®é™…éƒ¨ç½²æ—¶ä¼šé€šè¿‡ASGIæœåŠ¡å™¨å¦‚Uvicornï¼‰
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)
```

## 4. è‡ªåŠ¨åŒ–ä¸æœºå™¨äººæµç¨‹è‡ªåŠ¨åŒ–(RPA)

### 4.1 RPAåŸºç¡€ä¸Pythonå®ç°
**[æ ‡è¯†: RPA-001]**

æœºå™¨äººæµç¨‹è‡ªåŠ¨åŒ–(RPA)æ˜¯ä½¿ç”¨è½¯ä»¶æœºå™¨äººè‡ªåŠ¨æ‰§è¡Œé‡å¤çš„ã€åŸºäºè§„åˆ™çš„ä»»åŠ¡ï¼š

#### PyAutoGUI - è‡ªåŠ¨åŒ–GUIæ“ä½œ

```python
# ä½¿ç”¨PyAutoGUIè‡ªåŠ¨åŒ–æ¡Œé¢æ“ä½œ
import pyautogui
import time
import keyboard

def automate_data_entry(data):
    """è‡ªåŠ¨åŒ–æ•°æ®å½•å…¥è¿‡ç¨‹"""
    print("å‡†å¤‡å¼€å§‹è‡ªåŠ¨åŒ–æ“ä½œï¼Œè¯·ç¡®ä¿ç›®æ ‡åº”ç”¨ç¨‹åºçª—å£å¯è§")
    print("3ç§’åå¼€å§‹...")
    time.sleep(3)
    
    try:
        for item in data:
            # ç‚¹å‡»è¾“å…¥æ¡†ä½ç½®ï¼ˆéœ€è¦æ ¹æ®å®é™…åº”ç”¨è°ƒæ•´åæ ‡ï¼‰
            pyautogui.click(x=500, y=300)  # å‡è®¾è¿™æ˜¯ç¬¬ä¸€ä¸ªè¾“å…¥æ¡†çš„ä½ç½®
            time.sleep(0.5)
            
            # è¾“å…¥æ•°æ®
            pyautogui.write(item["name"])
            time.sleep(0.5)
            
            # æŒ‰Tabé”®ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªè¾“å…¥æ¡†
            pyautogui.press('tab')
            time.sleep(0.5)
            
            pyautogui.write(str(item["value"]))
            time.sleep(0.5)
            
            # æŒ‰Enteré”®æäº¤
            pyautogui.press('enter')
            time.sleep(1)  # ç­‰å¾…æ“ä½œå®Œæˆ
            
            # æ£€æŸ¥æ˜¯å¦æŒ‰ä¸‹ESCé”®ä»¥ä¸­æ–­æ“ä½œ
            if keyboard.is_pressed('esc'):
                print("è‡ªåŠ¨åŒ–æ“ä½œå·²ä¸­æ–­")
                break
        
        print("æ‰€æœ‰æ•°æ®å½•å…¥å®Œæˆ")
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
        # ä¿å­˜å½“å‰é¼ æ ‡ä½ç½®ä»¥ä¾›è°ƒè¯•
        print(f"å½“å‰é¼ æ ‡ä½ç½®: {pyautogui.position()}")

# ç¤ºä¾‹æ•°æ®
if __name__ == "__main__":
    test_data = [
        {"name": "äº§å“A", "value": 100},
        {"name": "äº§å“B", "value": 200},
        {"name": "äº§å“C", "value": 300}
    ]
    
    print("æç¤º: æŒ‰ESCé”®å¯ä»¥éšæ—¶ä¸­æ–­è‡ªåŠ¨åŒ–æ“ä½œ")
    automate_data_entry(test_data)
```

#### Selenium - ç½‘é¡µè‡ªåŠ¨åŒ–

```python
# ä½¿ç”¨Seleniumè‡ªåŠ¨åŒ–ç½‘é¡µæ“ä½œ
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

def automate_web_task():
    """è‡ªåŠ¨åŒ–ç½‘é¡µä»»åŠ¡"""
    # åˆå§‹åŒ–WebDriverï¼ˆéœ€è¦å®‰è£…å¯¹åº”çš„æµè§ˆå™¨é©±åŠ¨ï¼‰
    # è¿™é‡Œä½¿ç”¨Chromeä½œä¸ºç¤ºä¾‹ï¼Œéœ€è¦ä¸‹è½½chromedriverå¹¶æ”¾åœ¨PATHä¸­
    driver = webdriver.Chrome()
    
    try:
        # æ‰“å¼€ç½‘é¡µ
        driver.get("https://www.example.com")
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        driver.implicitly_wait(10)  # éšå¼ç­‰å¾…
        
        # æŸ¥æ‰¾å¹¶å¡«å†™è¡¨å•
        # æ³¨æ„ï¼šä¸‹é¢çš„é€‰æ‹©å™¨éœ€è¦æ ¹æ®å®é™…ç½‘é¡µè°ƒæ•´
        search_box = driver.find_element(By.NAME, "q")
        search_box.send_keys("Pythonè‡ªåŠ¨åŒ–")
        search_box.send_keys(Keys.RETURN)
        
        # æ˜¾å¼ç­‰å¾…æœç´¢ç»“æœåŠ è½½
        wait = WebDriverWait(driver, 10)
        results = wait.until(EC.presence_of_all_elements_located(
            (By.CSS_SELECTOR, "div.g")))
        
        # æå–å¹¶æ‰“å°ç»“æœ
        print(f"æ‰¾åˆ°{len(results)}ä¸ªç»“æœ:")
        for i, result in enumerate(results[:5]):  # åªæ‰“å°å‰5ä¸ªç»“æœ
            try:
                title = result.find_element(By.TAG_NAME, "h3").text
                link = result.find_element(By.TAG_NAME, "a").get_attribute("href")
                print(f"{i+1}. {title}")
                print(f"   {link}")
            except Exception as e:
                print(f"æ— æ³•æå–ç»“æœ {i+1}: {str(e)}")
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´ä»¥ä¾¿è§‚å¯Ÿ
        time.sleep(5)
        
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
    finally:
        # å…³é—­æµè§ˆå™¨
        driver.quit()

if __name__ == "__main__":
    automate_web_task()
```

### 4.2 æ™ºèƒ½è‡ªåŠ¨åŒ–ä¸æœºå™¨å­¦ä¹ ç»“åˆ
**[æ ‡è¯†: RPA-002]**

å°†æœºå™¨å­¦ä¹ ä¸RPAç»“åˆï¼Œå®ç°æ›´æ™ºèƒ½çš„è‡ªåŠ¨åŒ–æµç¨‹ï¼š

```python
# ç»“åˆOCRå’Œæœºå™¨å­¦ä¹ çš„æ™ºèƒ½æ–‡æ¡£å¤„ç†ç¤ºä¾‹
import pytesseract
from PIL import Image
import cv2
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import os

def extract_text_from_image(image_path):
    """ä½¿ç”¨OCRä»å›¾åƒä¸­æå–æ–‡æœ¬"""
    # è¯»å–å›¾åƒ
    img = cv2.imread(image_path)
    
    # å›¾åƒé¢„å¤„ç†
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    # åº”ç”¨è‡ªé€‚åº”é˜ˆå€¼
    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                  cv2.THRESH_BINARY, 11, 2)
    
    # ä½¿ç”¨Tesseractæå–æ–‡æœ¬
    text = pytesseract.image_to_string(thresh, lang='chi_sim+eng')
    
    return text

def classify_documents(document_folder, n_clusters=3):
    """ä»æ–‡æ¡£å›¾åƒä¸­æå–æ–‡æœ¬å¹¶è¿›è¡Œèšç±»åˆ†ç±»"""
    documents = []
    filenames = []
    
    # å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å›¾åƒ
    for filename in os.listdir(document_folder):
        if filename.endswith(('.png', '.jpg', '.jpeg')):
            filepath = os.path.join(document_folder, filename)
            print(f"å¤„ç†æ–‡ä»¶: {filename}")
            
            try:
                # æå–æ–‡æœ¬
                text = extract_text_from_image(filepath)
                documents.append(text)
                filenames.append(filename)
                print(f"  æå–åˆ°æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
            except Exception as e:
                print(f"  å¤„ç†å¤±è´¥: {str(e)}")
    
    if not documents:
        print("æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡æ¡£")
        return
    
    # ä½¿ç”¨TF-IDFå‘é‡åŒ–æ–‡æœ¬
    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
    X = vectorizer.fit_transform(documents)
    
    # ä½¿ç”¨K-meansè¿›è¡Œèšç±»
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    labels = kmeans.fit_predict(X)
    
    # æ˜¾ç¤ºç»“æœ
    print("\næ–‡æ¡£åˆ†ç±»ç»“æœ:")
    results = []
    for i, label in enumerate(labels):
        print(f"{filenames[i]} - ç±»åˆ« {label}")
        results.append({"æ–‡ä»¶å": filenames[i], "ç±»åˆ«": label, "æ–‡æœ¬": documents[i][:100] + "..."})
    
    # åˆ›å»ºç»“æœDataFrame
    df = pd.DataFrame(results)
    
    # æ˜¾ç¤ºæ¯ä¸ªç±»åˆ«çš„ä»£è¡¨æ€§è¯è¯­
    print("\nå„ç±»åˆ«ä»£è¡¨æ€§è¯è¯­:")
    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
    terms = vectorizer.get_feature_names_out()
    
    for i in range(n_clusters):
        print(f"ç±»åˆ« {i}:")
        for ind in order_centroids[i, :10]:  # æ˜¾ç¤ºå‰10ä¸ªä»£è¡¨æ€§è¯è¯­
            print(f"   {terms[ind]}")
    
    return df

if __name__ == "__main__":
    # éœ€è¦å®‰è£…ä»¥ä¸‹ä¾èµ–ï¼š
    # pip install pytesseract opencv-python pillow pandas scikit-learn
    # è¿˜éœ€è¦å®‰è£…Tesseract OCRå¼•æ“å¹¶é…ç½®è·¯å¾„
    # pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'  # Windowsç¤ºä¾‹
    
    document_folder = "documents"  # åŒ…å«æ–‡æ¡£å›¾åƒçš„æ–‡ä»¶å¤¹
    df = classify_documents(document_folder)
    
    if df is not None:
        print("\nç»“æœä¿å­˜åˆ° results.csv")
        df.to_csv("results.csv", index=False, encoding='utf-8-sig')
```

## 5. åˆ†å¸ƒå¼è®¡ç®—ä¸äº‘åŸç”Ÿåº”ç”¨

### 5.1 Pythonåˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶
**[æ ‡è¯†: CLOUD-001]**

Pythonæä¾›äº†å¤šç§åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡æ•°æ®å¤„ç†ï¼š

#### Dask - çµæ´»çš„å¹¶è¡Œè®¡ç®—åº“

```python
# ä½¿ç”¨Daskè¿›è¡Œå¹¶è¡Œæ•°æ®å¤„ç†
import dask.dataframe as dd
import pandas as pd
import numpy as np
import time

def demonstrate_dask_performance():
    """å±•ç¤ºDaskä¸Pandasæ€§èƒ½å¯¹æ¯”"""
    # åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é€šå¸¸æ˜¯å¤§å‹CSVæ–‡ä»¶ï¼‰
    print("ç”Ÿæˆç¤ºä¾‹æ•°æ®...")
    np.random.seed(42)
    n_rows = 10_000_000  # 1åƒä¸‡è¡Œæ•°æ®
    
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶è·¯å¾„
    file_path = "large_data.csv"
    
    # åˆ†å—ç”Ÿæˆå¹¶å†™å…¥æ•°æ®ï¼Œé¿å…å†…å­˜ä¸è¶³
    chunk_size = 1_000_000
    chunks = []
    
    for i in range(0, n_rows, chunk_size):
        chunk = pd.DataFrame({
            'id': range(i, min(i + chunk_size, n_rows)),
            'value1': np.random.normal(0, 1, min(chunk_size, n_rows - i)),
            'value2': np.random.normal(5, 2, min(chunk_size, n_rows - i)),
            'category': np.random.choice(['A', 'B', 'C', 'D'], min(chunk_size, n_rows - i))
        })
        chunks.append(chunk)
        
        # å†™å…¥CSVï¼Œç¬¬ä¸€ä¸ªå—åŒ…å«è¡¨å¤´
        if i == 0:
            chunk.to_csv(file_path, index=False)
        else:
            chunk.to_csv(file_path, index=False, header=False, mode='a')
    
    print(f"æ•°æ®ç”Ÿæˆå®Œæˆï¼Œå…± {n_rows} è¡Œ")
    
    # ä½¿ç”¨Pandaså¤„ç†ï¼ˆå°æ ·æœ¬ï¼‰
    print("\nä½¿ç”¨Pandaså¤„ç†ï¼ˆä»…è¯»å–å‰100ä¸‡è¡Œï¼‰:")
    start_time = time.time()
    
    # åªè¯»å–å‰100ä¸‡è¡Œï¼Œé¿å…å†…å­˜é—®é¢˜
    df_pandas = pd.read_csv(file_path, nrows=1_000_000)
    result_pandas = df_pandas.groupby('category').agg({
        'value1': ['mean', 'sum', 'std'],
        'value2': ['mean', 'sum']
    })
    
    pandas_time = time.time() - start_time
    print(f"Pandaså¤„ç†æ—¶é—´: {pandas_time:.2f} ç§’")
    print("Pandasç»“æœ:")
    print(result_pandas)
    
    # ä½¿ç”¨Daskå¤„ç†
    print("\nä½¿ç”¨Daskå¤„ç†ï¼ˆå…¨éƒ¨æ•°æ®ï¼‰:")
    start_time = time.time()
    
    # è¯»å–CSVæ–‡ä»¶ï¼Œåˆ›å»ºDask DataFrame
    df_dask = dd.read_csv(file_path)
    
    # æ‰§è¡Œç›¸åŒçš„èšåˆæ“ä½œ
    result_dask = df_dask.groupby('category').agg({
        'value1': ['mean', 'sum', 'std'],
        'value2': ['mean', 'sum']
    }).compute()  # compute()è§¦å‘å®é™…è®¡ç®—
    
    dask_time = time.time() - start_time
    print(f"Daskå¤„ç†æ—¶é—´: {dask_time:.2f} ç§’")
    print("Daskç»“æœ:")
    print(result_dask)
    
    # æ¯”è¾ƒç»“æœ
    print("\næ€§èƒ½å¯¹æ¯”:")
    print(f"Pandasï¼ˆ100ä¸‡è¡Œï¼‰: {pandas_time:.2f} ç§’")
    print(f"Daskï¼ˆ1000ä¸‡è¡Œï¼‰: {dask_time:.2f} ç§’")
    print(f"Daskå¤„ç†å®Œæ•´æ•°æ®é›†æ¯”Pandaså¤„ç†10%æ•°æ®å¤šèŠ±è´¹äº† {dask_time/pandas_time:.2f}x æ—¶é—´")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    import os
    os.remove(file_path)
    print("\nä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")

if __name__ == "__main__":
    # éœ€è¦å®‰è£…ï¼špip install dask pandas numpy
    demonstrate_dask_performance()
```

#### PySpark - Apache Sparkçš„Python API

```python
# ä½¿ç”¨PySparkè¿›è¡Œåˆ†å¸ƒå¼æ•°æ®å¤„ç†
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, sum, stddev
import time
import os

def demonstrate_pyspark():
    """å±•ç¤ºPySparkåˆ†å¸ƒå¼æ•°æ®å¤„ç†"""
    # åˆ›å»ºSparkSession
    spark = SparkSession.builder \
        .appName("PySpark Example") \
        .master("local[*]")  # ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„CPUæ ¸å¿ƒ
        .config("spark.sql.shuffle.partitions", "8")  # è®¾ç½®shuffleåˆ†åŒºæ•°
        .getOrCreate()
    
    print("Sparkä¼šè¯åˆ›å»ºæˆåŠŸ")
    
    # æ•°æ®è·¯å¾„
    file_path = "large_data.csv"
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ç”Ÿæˆ
    if not os.path.exists(file_path):
        print("ç”Ÿæˆç¤ºä¾‹æ•°æ®...")
        # è¿™é‡Œå¯ä»¥å¤ç”¨å‰é¢Daskç¤ºä¾‹ä¸­çš„æ•°æ®ç”Ÿæˆä»£ç 
        import pandas as pd
        import numpy as np
        
        n_rows = 10_000_000
        chunk_size = 1_000_000
        
        for i in range(0, n_rows, chunk_size):
            chunk = pd.DataFrame({
                'id': range(i, min(i + chunk_size, n_rows)),
                'value1': np.random.normal(0, 1, min(chunk_size, n_rows - i)),
                'value2': np.random.normal(5, 2, min(chunk_size, n_rows - i)),
                'category': np.random.choice(['A', 'B', 'C', 'D'], min(chunk_size, n_rows - i))
            })
            
            if i == 0:
                chunk.to_csv(file_path, index=False)
            else:
                chunk.to_csv(file_path, index=False, header=False, mode='a')
    
    print(f"å¼€å§‹ä½¿ç”¨PySparkå¤„ç†æ•°æ®")
    start_time = time.time()
    
    # è¯»å–CSVæ–‡ä»¶
    df = spark.read.csv(file_path, header=True, inferSchema=True)
    
    # æ˜¾ç¤ºæ•°æ®ç»“æ„
    print("æ•°æ®ç»“æ„:")
    df.printSchema()
    
    # æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®
    print("å‰5è¡Œæ•°æ®:")
    df.show(5)
    
    # æ‰§è¡Œèšåˆæ“ä½œ
    print("æ‰§è¡Œèšåˆæ“ä½œ...")
    result = df.groupBy("category").agg(
        avg("value1").alias("avg_value1"),
        sum("value1").alias("sum_value1"),
        stddev("value1").alias("std_value1"),
        avg("value2").alias("avg_value2"),
        sum("value2").alias("sum_value2")
    )
    
    # æ˜¾ç¤ºç»“æœ
    print("èšåˆç»“æœ:")
    result.show()
    
    # è®¡ç®—æ•°æ®å¤„ç†æ—¶é—´
    pyspark_time = time.time() - start_time
    print(f"PySparkå¤„ç†æ—¶é—´: {pyspark_time:.2f} ç§’")
    
    # å¯ä»¥æ‰§è¡Œæ›´å¤æ‚çš„æ“ä½œ
    print("æ‰§è¡Œæ›´å¤æ‚çš„åˆ†æ...")
    
    # è¿‡æ»¤å’Œæ’åºç¤ºä¾‹
    filtered_df = df.filter(col("value1") > 1.0)
    sorted_df = filtered_df.orderBy(col("value2").desc())
    
    print(f"å€¼å¤§äº1.0çš„è®°å½•æ•°: {filtered_df.count()}")
    print("è¿™äº›è®°å½•ä¸­value2æœ€å¤§çš„å‰3æ¡:")
    sorted_df.select("id", "value1", "value2", "category").show(3)
    
    # å…³é—­Sparkä¼šè¯
    spark.stop()
    print("Sparkä¼šè¯å·²å…³é—­")

if __name__ == "__main__":
    # éœ€è¦å®‰è£…ï¼špip install pyspark
    demonstrate_pyspark()
```

### 5.2 å®¹å™¨åŒ–ä¸å¾®æœåŠ¡æ¶æ„
**[æ ‡è¯†: CLOUD-002]**

Pythonåº”ç”¨çš„å®¹å™¨åŒ–å’Œå¾®æœåŠ¡æ¶æ„å®ç°ï¼š

#### Dockerå®¹å™¨åŒ–Pythonåº”ç”¨

```dockerfile
# ç®€å•çš„Python Webåº”ç”¨Dockerfileç¤ºä¾‹
FROM python:3.9-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…ä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8000

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

# è¿è¡Œåº”ç”¨
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```python
# FastAPIå¾®æœåŠ¡ç¤ºä¾‹ï¼ˆapp/main.pyï¼‰
from fastapi import FastAPI, Depends, HTTPException
from sqlalchemy import create_engine, Column, Integer, String, Float
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
from pydantic import BaseModel
from typing import List
import os

# è·å–æ•°æ®åº“URLï¼ˆä»ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼ï¼‰
DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./items.db")

# åˆ›å»ºæ•°æ®åº“å¼•æ“å’Œä¼šè¯
engine = create_engine(
    DATABASE_URL, connect_args={"check_same_thread": False} if "sqlite" in DATABASE_URL else {}
)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# åˆ›å»ºæ•°æ®åº“æ¨¡å‹
Base = declarative_base()

class ItemDB(Base):
    __tablename__ = "items"
    
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, index=True)
    description = Column(String, nullable=True)
    price = Column(Float)
    category = Column(String)

# åˆ›å»ºæ•°æ®åº“è¡¨
Base.metadata.create_all(bind=engine)

# ä¾èµ–é¡¹ï¼šè·å–æ•°æ®åº“ä¼šè¯
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# Pydanticæ¨¡å‹ç”¨äºè¯·æ±‚å’Œå“åº”
class ItemBase(BaseModel):
    name: str
    description: str = None
    price: float
    category: str

class ItemCreate(ItemBase):
    pass

class Item(ItemBase):
    id: int
    
    class Config:
        orm_mode = True

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="ç‰©å“ç®¡ç†API",
    description="ä¸€ä¸ªç”¨äºç®¡ç†ç‰©å“çš„ç®€å•å¾®æœåŠ¡",
    version="1.0.0"
)

# APIç«¯ç‚¹
@app.post("/items/", response_model=Item)
def create_item(item: ItemCreate, db: Session = Depends(get_db)):
    db_item = ItemDB(**item.dict())
    db.add(db_item)
    db.commit()
    db.refresh(db_item)
    return db_item

@app.get("/items/", response_model=List[Item])
def read_items(skip: int = 0, limit: int = 100, category: str = None, db: Session = Depends(get_db)):
    query = db.query(ItemDB)
    if category:
        query = query.filter(ItemDB.category == category)
    items = query.offset(skip).limit(limit).all()
    return items

@app.get("/items/{item_id}", response_model=Item)
def read_item(item_id: int, db: Session = Depends(get_db)):
    db_item = db.query(ItemDB).filter(ItemDB.id == item_id).first()
    if db_item is None:
        raise HTTPException(status_code=404, detail="ç‰©å“ä¸å­˜åœ¨")
    return db_item

@app.put("/items/{item_id}", response_model=Item)
def update_item(item_id: int, item: ItemCreate, db: Session = Depends(get_db)):
    db_item = db.query(ItemDB).filter(ItemDB.id == item_id).first()
    if db_item is None:
        raise HTTPException(status_code=404, detail="ç‰©å“ä¸å­˜åœ¨")
    
    for key, value in item.dict().items():
        setattr(db_item, key, value)
    
    db.commit()
    db.refresh(db_item)
    return db_item

@app.delete("/items/{item_id}")
def delete_item(item_id: int, db: Session = Depends(get_db)):
    db_item = db.query(ItemDB).filter(ItemDB.id == item_id).first()
    if db_item is None:
        raise HTTPException(status_code=404, detail="ç‰©å“ä¸å­˜åœ¨")
    
    db.delete(db_item)
    db.commit()
    return {"detail": "ç‰©å“å·²åˆ é™¤"}

@app.get("/categories/")
def get_categories(db: Session = Depends(get_db)):
    categories = db.query(ItemDB.category).distinct().all()
    return {"categories": [cat[0] for cat in categories]}
```

#### Docker Composeç¼–æ’å¤šå®¹å™¨åº”ç”¨

```yaml
# docker-compose.ymlç¤ºä¾‹
version: '3.8'

services:
  # FastAPIåº”ç”¨
  web:
    build: .
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    volumes:
      - .:/app
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://admin:password@db/example_db
    depends_on:
      - db
    restart: always

  # PostgreSQLæ•°æ®åº“
  db:
    image: postgres:13
    volumes:
      - postgres_data:/var/lib/postgresql/data/
    environment:
      - POSTGRES_USER=admin
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=example_db
    ports:
      - "5432:5432"
    restart: always

volumes:
  postgres_data:
```

## 6. Pythonæ–°å…´æŠ€æœ¯è¶‹åŠ¿ä¸å±•æœ›

### 6.1 é‡å­è®¡ç®—å•†ä¸šåŒ–å‰æ™¯
**[æ ‡è¯†: FUTURE-001]**

é‡å­è®¡ç®—æ­£é€æ­¥ä»å®éªŒå®¤èµ°å‘å•†ä¸šåº”ç”¨ï¼š

- **äº‘é‡å­è®¡ç®—æœåŠ¡**ï¼šAWS Braketã€IBM Quantumç­‰äº‘å¹³å°æä¾›é‡å­è®¡ç®—è®¿é—®
- **è¡Œä¸šè§£å†³æ–¹æ¡ˆ**ï¼šé‡‘èã€åˆ¶è¯ã€ææ–™ç§‘å­¦ç­‰é¢†åŸŸçš„é‡å­ç®—æ³•åº”ç”¨
- **é‡å­ä¼˜åŠ¿éªŒè¯**ï¼šç‰¹å®šé—®é¢˜ä¸Šé‡å­è®¡ç®—è¶…è¶Šç»å…¸è®¡ç®—çš„æ¡ˆä¾‹ä¸æ–­æ¶Œç°
- **é‡å­å®‰å…¨é€šä¿¡**ï¼šåŸºäºé‡å­å¯†é’¥åˆ†å‘çš„å®‰å…¨é€šä¿¡ç½‘ç»œå»ºè®¾

#### ä½¿ç”¨Qiskitè¿›è¡Œé‡å­è®¡ç®—ç¤ºä¾‹

```python
# ä½¿ç”¨Qiskitè¿›è¡Œé‡å­è®¡ç®—å…¥é—¨ç¤ºä¾‹
from qiskit import QuantumCircuit, Aer, execute
from qiskit.visualization import plot_histogram, plot_bloch_multivector
import numpy as np
import matplotlib.pyplot as plt

def quantum_hello_world():
    """åˆ›å»ºä¸€ä¸ªç®€å•çš„é‡å­ç”µè·¯å¹¶æ‰§è¡Œ"""
    # åˆ›å»ºä¸€ä¸ªåŒ…å«2ä¸ªé‡å­æ¯”ç‰¹å’Œ2ä¸ªç»å…¸æ¯”ç‰¹çš„é‡å­ç”µè·¯
    qc = QuantumCircuit(2, 2)
    
    # åœ¨ç¬¬ä¸€ä¸ªé‡å­æ¯”ç‰¹ä¸Šåº”ç”¨Hadamardé—¨ï¼Œåˆ›å»ºå åŠ æ€
    qc.h(0)
    
    # åœ¨ä¸¤ä¸ªé‡å­æ¯”ç‰¹ä¹‹é—´åº”ç”¨CNOTé—¨ï¼Œåˆ›å»ºçº ç¼ æ€
    qc.cx(0, 1)
    
    # æµ‹é‡é‡å­æ¯”ç‰¹å¹¶å°†ç»“æœå­˜å‚¨åˆ°ç»å…¸æ¯”ç‰¹
    qc.measure([0, 1], [0, 1])
    
    # å¯è§†åŒ–é‡å­ç”µè·¯
    print("é‡å­ç”µè·¯:")
    print(qc.draw())
    
    # ä½¿ç”¨Aerçš„qasm_simulatoræ¨¡æ‹Ÿæ‰§è¡Œé‡å­ç”µè·¯
    simulator = Aer.get_backend('qasm_simulator')
    
    # æ‰§è¡Œç”µè·¯1000æ¬¡
    job = execute(qc, simulator, shots=1000)
    
    # è·å–ç»“æœ
    result = job.result()
    
    # ç»Ÿè®¡æµ‹é‡ç»“æœ
    counts = result.get_counts(qc)
    print("\næµ‹é‡ç»“æœç»Ÿè®¡:")
    print(counts)
    
    # ç»˜åˆ¶ç»“æœç›´æ–¹å›¾
    plot_histogram(counts)
    plt.title('çº ç¼ æ€æµ‹é‡ç»“æœ')
    plt.savefig('quantum_results.png')
    print("\nç»“æœç›´æ–¹å›¾å·²ä¿å­˜ä¸º 'quantum_results.png'")
    
    return qc, counts

def quantum_teleportation():
    """é‡å­éšå½¢ä¼ æ€ç¤ºä¾‹ - é‡å­ä¿¡æ¯çš„ä¼ è¾“åè®®"""
    # åˆ›å»ºä¸€ä¸ªåŒ…å«3ä¸ªé‡å­æ¯”ç‰¹å’Œ2ä¸ªç»å…¸æ¯”ç‰¹çš„é‡å­ç”µè·¯
    qc = QuantumCircuit(3, 2)
    
    # è®¾ç½®åˆå§‹çŠ¶æ€ï¼ˆå‡è®¾æˆ‘ä»¬æƒ³è¦ä¼ è¾“çš„é‡å­æ€ï¼‰
    # è¿™é‡Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªä»»æ„çš„é‡å­æ€ |ÏˆâŸ© = Î±|0âŸ© + Î²|1âŸ©
    # æˆ‘ä»¬ä½¿ç”¨æ—‹è½¬é—¨æ¥åˆ›å»ºè¿™ä¸ªçŠ¶æ€
    qc.ry(np.pi/4, 0)  # åœ¨ç¬¬ä¸€ä¸ªé‡å­æ¯”ç‰¹ä¸Šåº”ç”¨RYæ—‹è½¬é—¨
    
    # ä¸ºäº†å¯è§†åŒ–åˆå§‹çŠ¶æ€ï¼Œæˆ‘ä»¬å…ˆè¿›è¡Œä¸€æ¬¡æµ‹é‡
    initial_state_circuit = qc.copy()
    initial_state_circuit.measure([0], [0])
    
    # åˆ›å»ºBellå¯¹ï¼ˆçº ç¼ æ€ï¼‰ç”¨äºé‡å­éšå½¢ä¼ æ€
    # åœ¨ç¬¬äºŒä¸ªå’Œç¬¬ä¸‰ä¸ªé‡å­æ¯”ç‰¹ä¹‹é—´åˆ›å»ºçº ç¼ 
    qc.h(1)
    qc.cx(1, 2)
    
    # æ‰§è¡ŒBellæµ‹é‡éƒ¨åˆ†
    qc.cx(0, 1)
    qc.h(0)
    qc.measure([0, 1], [0, 1])
    
    # æ ¹æ®æµ‹é‡ç»“æœåº”ç”¨ç›¸åº”çš„é—¨åˆ°ç›®æ ‡é‡å­æ¯”ç‰¹ï¼ˆç¬¬ä¸‰ä¸ªï¼‰
    # è¿™éƒ¨åˆ†åœ¨å®é™…é‡å­è®¡ç®—ä¸­æ˜¯åŸºäºç»å…¸é€šä¿¡è¿›è¡Œçš„
    qc.x(2).c_if(1, 1)  # å¦‚æœç»å…¸æ¯”ç‰¹1ä¸º1ï¼Œåˆ™åº”ç”¨Xé—¨
    qc.z(2).c_if(0, 1)  # å¦‚æœç»å…¸æ¯”ç‰¹0ä¸º1ï¼Œåˆ™åº”ç”¨Zé—¨
    
    # å¯è§†åŒ–å®Œæ•´çš„é‡å­ç”µè·¯
    print("é‡å­éšå½¢ä¼ æ€ç”µè·¯:")
    print(qc.draw())
    
    # æ¨¡æ‹Ÿæ‰§è¡Œ
    simulator = Aer.get_backend('qasm_simulator')
    job = execute(qc, simulator, shots=1000)
    result = job.result()
    counts = result.get_counts(qc)
    
    # æˆ‘ä»¬åªå…³å¿ƒç¬¬ä¸‰ä¸ªé‡å­æ¯”ç‰¹çš„æœ€ç»ˆçŠ¶æ€
    # ä½†ç”±äºæˆ‘ä»¬ä½¿ç”¨äº†æµ‹é‡ï¼Œæˆ‘ä»¬éœ€è¦ä»è®¡æ•°ä¸­æå–ç¬¬ä¸‰ä¸ªæ¯”ç‰¹çš„ä¿¡æ¯
    print("\né‡å­éšå½¢ä¼ æ€ç»“æœç»Ÿè®¡:")
    print(counts)
    
    # ä¸ºäº†æ¯”è¾ƒåˆå§‹çŠ¶æ€å’Œæœ€ç»ˆçŠ¶æ€ï¼Œæˆ‘ä»¬ä¹Ÿæ‰§è¡Œåˆå§‹çŠ¶æ€ç”µè·¯
    initial_job = execute(initial_state_circuit, simulator, shots=1000)
    initial_result = initial_job.result()
    initial_counts = initial_result.get_counts(initial_state_circuit)
    
    print("\nåˆå§‹çŠ¶æ€æµ‹é‡ç»“æœ:")
    print(initial_counts)
    
    return qc, counts, initial_counts

if __name__ == "__main__":
    print("=== é‡å­Hello World ===")
    qc1, counts1 = quantum_hello_world()
    
    print("\n=== é‡å­éšå½¢ä¼ æ€ç¤ºä¾‹ ===")
    qc2, teleport_counts, initial_counts = quantum_teleportation()
    
    print("\næç¤ºï¼šè¦è¿è¡Œæ­¤ç¤ºä¾‹ï¼Œéœ€è¦å®‰è£…Qiskit:")
    print("pip install qiskit qiskit-terra")

### 6.2 AIä¸è‡ªåŠ¨åŒ–æ·±åº¦èåˆ
**[æ ‡è¯†: FUTURE-002]**

äººå·¥æ™ºèƒ½ä¸è‡ªåŠ¨åŒ–æŠ€æœ¯çš„èåˆå°†å¸¦æ¥æ–°çš„ç”Ÿäº§åŠ›é©å‘½ï¼š

- **æ™ºèƒ½RPA**ï¼šç»“åˆæœºå™¨å­¦ä¹ çš„ä¸‹ä¸€ä»£è‡ªåŠ¨åŒ–æŠ€æœ¯
- **æ•°å­—å­ªç”Ÿ**ï¼šç‰©ç†ç³»ç»Ÿçš„æ•°å­—åŒ–æ˜ å°„ï¼Œç”¨äºæ¨¡æ‹Ÿå’Œä¼˜åŒ–
- **è‡ªä¸»ç³»ç»Ÿ**ï¼šå…·å¤‡è‡ªæˆ‘å†³ç­–å’Œé€‚åº”èƒ½åŠ›çš„æ™ºèƒ½ç³»ç»Ÿ
- **äººæœºåä½œ**ï¼šAIè¾…åŠ©äººç±»å·¥ä½œï¼Œæé«˜æ•ˆç‡å’Œåˆ›é€ åŠ›

### 6.3 å¯æŒç»­å‘å±•ä¸ç»¿è‰²è®¡ç®—
**[æ ‡è¯†: FUTURE-003]**

Pythonåœ¨å¯æŒç»­å‘å±•å’Œç»¿è‰²è®¡ç®—ä¸­çš„åº”ç”¨ï¼š

```python
# ç®€å•çš„èƒ½æºä½¿ç”¨ç›‘æ§å’Œä¼˜åŒ–ç¤ºä¾‹
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

def monitor_energy_consumption():
    """æ¨¡æ‹Ÿèƒ½æºæ¶ˆè€—ç›‘æ§ä¸ä¼˜åŒ–"""
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®ï¼ˆå®é™…åº”ç”¨ä¸­ä¼šä»ä¼ æ„Ÿå™¨æˆ–ç³»ç»Ÿæ—¥å¿—è·å–ï¼‰
    np.random.seed(42)
    n_days = 365
    
    # åˆ›å»ºæ—¶é—´ç´¢å¼•
    dates = pd.date_range(start='2023-01-01', periods=n_days, freq='D')
    
    # ç”ŸæˆåŸºç¡€èƒ½æºæ¶ˆè€—æ•°æ®ï¼ˆè€ƒè™‘å­£èŠ‚æ€§å’Œæ—¥å¸¸æ¨¡å¼ï¼‰
    base_consumption = 100  # åŸºç¡€æ¶ˆè€—é‡
    seasonal_factor = 20 * np.sin(2 * np.pi * np.arange(n_days) / 365)  # å­£èŠ‚æ€§å˜åŒ–
    weekend_factor = np.array([1.5 if date.dayofweek >= 5 else 1.0 for date in dates])  # å‘¨æœ«æ•ˆåº”
    temperature = 20 + 10 * np.sin(2 * np.pi * np.arange(n_days) / 365) + np.random.normal(0, 2, n_days)  # æ¸©åº¦
    occupancy = np.random.poisson(10, n_days)  # äººå‘˜æ•°é‡
    equipment_usage = np.random.uniform(0.5, 1.5, n_days)  # è®¾å¤‡ä½¿ç”¨æƒ…å†µ
    
    # è®¡ç®—æ€»èƒ½æºæ¶ˆè€—
    energy_consumption = base_consumption + seasonal_factor * weekend_factor + \
                         (25 - temperature) * 2 + occupancy * 2 + equipment_usage * 10 + \
                         np.random.normal(0, 5, n_days)  # éšæœºå™ªå£°
    
    # ç¡®ä¿èƒ½æºæ¶ˆè€—ä¸ºæ­£å€¼
    energy_consumption = np.maximum(energy_consumption, 0)
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame({
        'date': dates,
        'energy': energy_consumption,
        'temperature': temperature,
        'occupancy': occupancy,
        'equipment_usage': equipment_usage,
        'day_of_week': dates.dayofweek,
        'month': dates.month
    })
    
    # æ·»åŠ æ˜¯å¦ä¸ºå‘¨æœ«çš„æ ‡å¿—
    df['is_weekend'] = df['day_of_week'] >= 5
    
    # æ•°æ®å¯è§†åŒ–
    plt.figure(figsize=(12, 6))
    plt.plot(df['date'], df['energy'])
    plt.title('æ¯æ—¥èƒ½æºæ¶ˆè€—')
    plt.xlabel('æ—¥æœŸ')
    plt.ylabel('èƒ½æºæ¶ˆè€— (kWh)')
    plt.grid(True)
    plt.tight_layout()
    plt.savefig('energy_consumption.png')
    
    # ç‰¹å¾å·¥ç¨‹
    features = ['temperature', 'occupancy', 'equipment_usage', 'day_of_week', 'month', 'is_weekend']
    X = df[features]
    y = df['energy']
    
    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # è®­ç»ƒé¢„æµ‹æ¨¡å‹
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    # é¢„æµ‹
    y_pred = model.predict(X_test)
    
    # è¯„ä¼°æ¨¡å‹
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    print(f"æ¨¡å‹RMSE: {rmse:.2f}")
    
    # ç‰¹å¾é‡è¦æ€§
    importance = pd.DataFrame({
        'feature': features,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nç‰¹å¾é‡è¦æ€§:")
    print(importance)
    
    # èŠ‚èƒ½å»ºè®®ç”Ÿæˆ
    print("\nèŠ‚èƒ½å»ºè®®:")
    
    # åˆ†æå‘¨æœ«vså·¥ä½œæ—¥æ¶ˆè€—
    weekend_avg = df[df['is_weekend']]['energy'].mean()
    weekday_avg = df[~df['is_weekend']]['energy'].mean()
    if weekend_avg > weekday_avg * 1.2:  # å¦‚æœå‘¨æœ«æ¶ˆè€—æ˜æ˜¾é«˜äºå·¥ä½œæ—¥
        print(f"1. å‘¨æœ«èƒ½æºæ¶ˆè€—(å¹³å‡ {weekend_avg:.2f} kWh)é«˜äºå·¥ä½œæ—¥(å¹³å‡ {weekday_avg:.2f} kWh)ï¼Œå»ºè®®æ£€æŸ¥å‘¨æœ«è®¾å¤‡ä½¿ç”¨æƒ…å†µ")
    
    # åˆ†ææ¸©åº¦å½±å“
    temp_corr = df[['energy', 'temperature']].corr().iloc[0, 1]
    print(f"2. èƒ½æºæ¶ˆè€—ä¸æ¸©åº¦ç›¸å…³ç³»æ•°: {temp_corr:.2f}ï¼Œå»ºè®®æ ¹æ®å­£èŠ‚è°ƒæ•´ç©ºè°ƒæ¸©åº¦è®¾ç½®")
    
    # åˆ†æè®¾å¤‡ä½¿ç”¨æƒ…å†µ
    high_usage_days = df[df['equipment_usage'] > df['equipment_usage'].quantile(0.8)]
    avg_high_usage = high_usage_days['energy'].mean()
    avg_normal_usage = df[df['equipment_usage'] <= df['equipment_usage'].quantile(0.8)]['energy'].mean()
    print(f"3. è®¾å¤‡é«˜ä½¿ç”¨ç‡æ—¥æœŸèƒ½æºæ¶ˆè€—(å¹³å‡ {avg_high_usage:.2f} kWh)é«˜äºæ­£å¸¸ä½¿ç”¨æ—¥æœŸ(å¹³å‡ {avg_normal_usage:.2f} kWh)ï¼Œå»ºè®®ä¼˜åŒ–è®¾å¤‡ä½¿ç”¨æ—¶é—´")
    
    # ç”Ÿæˆä¼˜åŒ–å»ºè®®
    optimized_consumption = df['energy'].copy()
    
    # åº”ç”¨å‘¨æœ«ä¼˜åŒ–
    optimized_consumption[df['is_weekend']] = df['energy'][df['is_weekend']] * 0.8  # å‡è®¾å‘¨æœ«ä¼˜åŒ–20%
    
    # åº”ç”¨æ¸©åº¦ç›¸å…³ä¼˜åŒ–
    if temp_corr < -0.3:  # å¦‚æœèƒ½æºæ¶ˆè€—ä¸æ¸©åº¦è´Ÿç›¸å…³ï¼ˆå¯èƒ½æ˜¯ä¾›æš–ç³»ç»Ÿï¼‰
        # å¯¹äºæ¸©åº¦è¾ƒé«˜çš„æ—¥å­ï¼Œé™ä½ä¾›æš–èƒ½è€—
        optimized_consumption[df['temperature'] > df['temperature'].median()] *= 0.9
    
    # è®¡ç®—æ½œåœ¨èŠ‚çœ
    total_consumption = df['energy'].sum()
    potential_savings = total_consumption - optimized_consumption.sum()
    savings_percentage = (potential_savings / total_consumption) * 100
    
    print(f"\næ½œåœ¨èŠ‚èƒ½æœºä¼š:")
    print(f"- å½“å‰å¹´åº¦æ€»æ¶ˆè€—: {total_consumption:.2f} kWh")
    print(f"- ä¼˜åŒ–åæ½œåœ¨æ€»æ¶ˆè€—: {optimized_consumption.sum():.2f} kWh")
    print(f"- æ½œåœ¨èŠ‚çœ: {potential_savings:.2f} kWh ({savings_percentage:.2f}%)")
    
    return df, model, importance

if __name__ == "__main__":
    monitor_energy_consumption()
```

### 6.4 Pythonåœ¨å‰æ²¿ç§‘å­¦ç ”ç©¶ä¸­çš„åº”ç”¨
**[æ ‡è¯†: FUTURE-004]**

Pythonåœ¨ç§‘å­¦ç ”ç©¶é¢†åŸŸçš„æœ€æ–°åº”ç”¨ï¼š

- **æ°”å€™å˜åŒ–ç ”ç©¶**ï¼šæ°”å€™æ¨¡å‹æ¨¡æ‹Ÿå’Œæ•°æ®åˆ†æ
- **ç”Ÿç‰©ä¿¡æ¯å­¦**ï¼šåŸºå› ç»„å­¦å’Œè›‹ç™½è´¨ç»„å­¦ç ”ç©¶
- **ç²’å­ç‰©ç†å­¦**ï¼šå¤§å‹å¼ºå­å¯¹æ’æœºæ•°æ®å¤„ç†
- **å¤ªç©ºæ¢ç´¢**ï¼šè¡Œæ˜Ÿæ•°æ®åˆ†æå’Œå¤©ä½“ç‰©ç†æ¨¡æ‹Ÿ
- **ç¥ç»ç§‘å­¦**ï¼šè„‘æœºæ¥å£å’Œç¥ç»ä¿¡å·å¤„ç†

## 7. AIä¼¦ç†ä¸è´Ÿè´£ä»»çš„AIå¼€å‘

### 7.1 AIä¼¦ç†æ ¸å¿ƒåŸåˆ™
**[æ ‡è¯†: ETHICS-001]**

éšç€AIæŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ï¼Œä¼¦ç†é—®é¢˜å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼š

- **å…¬å¹³æ€§ä¸æ— æ­§è§†**ï¼šç¡®ä¿AIç³»ç»Ÿä¸æ­§è§†ä»»ä½•ç¾¤ä½“æˆ–ä¸ªäºº
- **é€æ˜åº¦ä¸å¯è§£é‡Šæ€§**ï¼šä½¿AIå†³ç­–è¿‡ç¨‹å¯è¢«ç†è§£å’ŒéªŒè¯
- **éšç§ä¿æŠ¤**ï¼šå°Šé‡å’Œä¿æŠ¤ç”¨æˆ·æ•°æ®éšç§
- **è´£ä»»å½’å±**ï¼šæ˜ç¡®AIç³»ç»Ÿå†³ç­–çš„è´£ä»»ä¸»ä½“
- **å®‰å…¨ä¸é²æ£’æ€§**ï¼šé˜²æ­¢AIç³»ç»Ÿè¢«æ»¥ç”¨æˆ–äº§ç”Ÿæ„å¤–åæœ

#### Pythonå®ç°AIå…¬å¹³æ€§å®¡è®¡å·¥å…·ç¤ºä¾‹

```python
# AIä¼¦ç†ä¸å…¬å¹³æ€§å®¡è®¡å·¥å…·ç¤ºä¾‹
import numpy as np
import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

class AIFairnessAuditor:
    """AIå…¬å¹³æ€§å®¡è®¡å·¥å…·ç±»ï¼Œç”¨äºè¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹çš„å…¬å¹³æ€§"""
    
    def __init__(self, model=None, X=None, y=None, protected_attributes=None):
        """åˆå§‹åŒ–å…¬å¹³æ€§å®¡è®¡å·¥å…·
        
        å‚æ•°:
        model: è¦è¯„ä¼°çš„æœºå™¨å­¦ä¹ æ¨¡å‹
        X: ç‰¹å¾æ•°æ®
        y: çœŸå®æ ‡ç­¾
        protected_attributes: å—ä¿æŠ¤å±æ€§çš„åç§°åˆ—è¡¨ï¼ˆå¦‚æ€§åˆ«ã€ç§æ—ç­‰ï¼‰
        """
        self.model = model
        self.X = X
        self.y = y
        self.protected_attributes = protected_attributes or []
        self.y_pred = None
        
    def generate_predictions(self):
        """ä½¿ç”¨æ¨¡å‹ç”Ÿæˆé¢„æµ‹ç»“æœ"""
        if self.model is not None and self.X is not None:
            try:
                self.y_pred = self.model.predict(self.X)
                print("é¢„æµ‹å·²ç”Ÿæˆ")
                return True
            except Exception as e:
                print(f"ç”Ÿæˆé¢„æµ‹æ—¶å‡ºé”™: {e}")
                return False
        return False
    
    def overall_performance(self):
        """è®¡ç®—æ¨¡å‹çš„æ•´ä½“æ€§èƒ½æŒ‡æ ‡"""
        if self.y is None or self.y_pred is None:
            print("éœ€è¦çœŸå®æ ‡ç­¾å’Œé¢„æµ‹ç»“æœ")
            return None
        
        print("\n=== æ•´ä½“æ¨¡å‹æ€§èƒ½ ===")
        print(classification_report(self.y, self.y_pred))
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(self.y, self.y_pred)
        
        # å¯è§†åŒ–æ··æ·†çŸ©é˜µ
        plt.figure(figsize=(10, 7))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.title('æ··æ·†çŸ©é˜µ')
        plt.savefig('confusion_matrix.png')
        print("æ··æ·†çŸ©é˜µå·²ä¿å­˜ä¸º: confusion_matrix.png")
        
        return classification_report(self.y, self.y_pred, output_dict=True)
    
    def demographic_parity(self, attribute, favorable_outcome=1):
        """è®¡ç®—äººå£ç»Ÿè®¡å¹³ä»·ï¼ˆDemographic Parityï¼‰
        
        äººå£ç»Ÿè®¡å¹³ä»·ï¼šä¸åŒå—ä¿æŠ¤ç¾¤ä½“è·å¾—æœ‰åˆ©ç»“æœçš„æ¦‚ç‡åº”è¯¥ç›¸ä¼¼
        """
        if attribute not in self.X.columns or self.y_pred is None:
            print(f"å±æ€§ {attribute} ä¸å­˜åœ¨æˆ–æ²¡æœ‰é¢„æµ‹ç»“æœ")
            return None
        
        results = {}
        groups = self.X[attribute].unique()
        
        print(f"\n=== äººå£ç»Ÿè®¡å¹³ä»·åˆ†æ - {attribute} ===")
        
        for group in groups:
            mask = self.X[attribute] == group
            group_size = mask.sum()
            favorable_predictions = (self.y_pred[mask] == favorable_outcome).sum()
            rate = favorable_predictions / group_size if group_size > 0 else 0
            
            results[group] = {
                'group_size': group_size,
                'favorable_predictions': favorable_predictions,
                'rate': rate
            }
            
            print(f"ç»„ {group}: æœ‰åˆ©ç»“æœç‡ = {rate:.4f} ({favorable_predictions}/{group_size})")
        
        # è®¡ç®—æœ€å¤§å·®å¼‚
        rates = [v['rate'] for v in results.values()]
        max_difference = max(rates) - min(rates)
        print(f"æœ€å¤§ç»„é—´å·®å¼‚: {max_difference:.4f}")
        
        # å¯è§†åŒ–ç»“æœ
        plt.figure(figsize=(10, 6))
        plt.bar(results.keys(), [v['rate'] for v in results.values()])
        plt.xlabel(attribute)
        plt.ylabel('æœ‰åˆ©ç»“æœç‡')
        plt.title(f'{attribute}çš„äººå£ç»Ÿè®¡å¹³ä»·åˆ†æ')
        plt.savefig(f'demographic_parity_{attribute}.png')
        print(f"äººå£ç»Ÿè®¡å¹³ä»·å¯è§†åŒ–å·²ä¿å­˜ä¸º: demographic_parity_{attribute}.png")
        
        return {'results': results, 'max_difference': max_difference}
    
    def equalized_odds(self, attribute, favorable_outcome=1):
        """è®¡ç®—ç­‰åŒ–èµ”ç‡ï¼ˆEqualized Oddsï¼‰
        
        ç­‰åŒ–èµ”ç‡ï¼šå¯¹äºçœŸå®çš„æœ‰åˆ©å’Œä¸åˆ©æƒ…å†µï¼Œä¸åŒç¾¤ä½“çš„çœŸé˜³æ€§ç‡å’Œå‡é˜³æ€§ç‡åº”è¯¥ç›¸ä¼¼
        """
        if attribute not in self.X.columns or self.y is None or self.y_pred is None:
            print(f"å±æ€§ {attribute} ä¸å­˜åœ¨æˆ–ç¼ºå°‘å¿…è¦çš„æ•°æ®")
            return None
        
        results = {}
        groups = self.X[attribute].unique()
        
        print(f"\n=== ç­‰åŒ–èµ”ç‡åˆ†æ - {attribute} ===")
        
        for group in groups:
            mask = self.X[attribute] == group
            group_y = self.y[mask]
            group_y_pred = self.y_pred[mask]
            
            # è®¡ç®—æ··æ·†çŸ©é˜µå…ƒç´ 
            tn, fp, fn, tp = confusion_matrix(group_y, group_y_pred).ravel()
            
            # è®¡ç®—çœŸé˜³æ€§ç‡å’Œå‡é˜³æ€§ç‡
            tpr = tp / (tp + fn) if (tp + fn) > 0 else 0
            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
            
            results[group] = {
                'tpr': tpr,  # çœŸé˜³æ€§ç‡
                'fpr': fpr,  # å‡é˜³æ€§ç‡
                'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn
            }
            
            print(f"ç»„ {group}:")
            print(f"  çœŸé˜³æ€§ç‡ (TPR): {tpr:.4f}")
            print(f"  å‡é˜³æ€§ç‡ (FPR): {fpr:.4f}")
        
        # è®¡ç®—ç»„é—´TPRå’ŒFPRå·®å¼‚
        tprs = [v['tpr'] for v in results.values()]
        fprs = [v['fpr'] for v in results.values()]
        
        max_tpr_diff = max(tprs) - min(tprs)
        max_fpr_diff = max(fprs) - min(fprs)
        
        print(f"TPRæœ€å¤§ç»„é—´å·®å¼‚: {max_tpr_diff:.4f}")
        print(f"FPRæœ€å¤§ç»„é—´å·®å¼‚: {max_fpr_diff:.4f}")
        
        # å¯è§†åŒ–ç»“æœ
        plt.figure(figsize=(12, 6))
        
        # TPRå¯¹æ¯”
        plt.subplot(1, 2, 1)
        plt.bar(results.keys(), [v['tpr'] for v in results.values()], color='green')
        plt.xlabel(attribute)
        plt.ylabel('çœŸé˜³æ€§ç‡ (TPR)')
        plt.title(f'{attribute}çš„TPRå¯¹æ¯”')
        
        # FPRå¯¹æ¯”
        plt.subplot(1, 2, 2)
        plt.bar(results.keys(), [v['fpr'] for v in results.values()], color='red')
        plt.xlabel(attribute)
        plt.ylabel('å‡é˜³æ€§ç‡ (FPR)')
        plt.title(f'{attribute}çš„FPRå¯¹æ¯”')
        
        plt.tight_layout()
        plt.savefig(f'equalized_odds_{attribute}.png')
        print(f"ç­‰åŒ–èµ”ç‡å¯è§†åŒ–å·²ä¿å­˜ä¸º: equalized_odds_{attribute}.png")
        
        return {
            'results': results,
            'max_tpr_difference': max_tpr_diff,
            'max_fpr_difference': max_fpr_diff
        }
    
    def predictive_equality(self, attribute, favorable_outcome=1):
        """è®¡ç®—é¢„æµ‹å¹³ç­‰æ€§ï¼ˆPredictive Equalityï¼‰
        
        é¢„æµ‹å¹³ç­‰æ€§ï¼šä¸åŒç¾¤ä½“çš„å‡é˜³æ€§ç‡åº”è¯¥ç›¸ä¼¼
        """
        if attribute not in self.X.columns or self.y is None or self.y_pred is None:
            print(f"å±æ€§ {attribute} ä¸å­˜åœ¨æˆ–ç¼ºå°‘å¿…è¦çš„æ•°æ®")
            return None
        
        results = {}
        groups = self.X[attribute].unique()
        
        print(f"\n=== é¢„æµ‹å¹³ç­‰æ€§åˆ†æ - {attribute} ===")
        
        for group in groups:
            mask = self.X[attribute] == group
            group_y = self.y[mask]
            group_y_pred = self.y_pred[mask]
            
            # è®¡ç®—æ··æ·†çŸ©é˜µå…ƒç´ 
            tn, fp, fn, tp = confusion_matrix(group_y, group_y_pred).ravel()
            
            # è®¡ç®—å‡é˜³æ€§ç‡
            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
            
            results[group] = {
                'fpr': fpr,
                'fp': fp, 'tn': tn
            }
            
            print(f"ç»„ {group}: å‡é˜³æ€§ç‡ (FPR) = {fpr:.4f}")
        
        # è®¡ç®—æœ€å¤§FPRå·®å¼‚
        fprs = [v['fpr'] for v in results.values()]
        max_fpr_diff = max(fprs) - min(fprs)
        
        print(f"FPRæœ€å¤§ç»„é—´å·®å¼‚: {max_fpr_diff:.4f}")
        
        return {'results': results, 'max_fpr_difference': max_fpr_diff}
    
    def run_full_audit(self, favorable_outcome=1):
        """è¿è¡Œå®Œæ•´çš„å…¬å¹³æ€§å®¡è®¡"""
        if not self.generate_predictions():
            print("æ— æ³•ç”Ÿæˆé¢„æµ‹ï¼Œå®¡è®¡ç»ˆæ­¢")
            return None
        
        # æ•´ä½“æ€§èƒ½
        overall = self.overall_performance()
        
        # å¯¹æ¯ä¸ªå—ä¿æŠ¤å±æ€§è¿›è¡Œå…¬å¹³æ€§åˆ†æ
        fairness_results = {}
        
        for attribute in self.protected_attributes:
            print(f"\n======== åˆ†æå—ä¿æŠ¤å±æ€§: {attribute} ========")
            
            dp_results = self.demographic_parity(attribute, favorable_outcome)
            eo_results = self.equalized_odds(attribute, favorable_outcome)
            pe_results = self.predictive_equality(attribute, favorable_outcome)
            
            fairness_results[attribute] = {
                'demographic_parity': dp_results,
                'equalized_odds': eo_results,
                'predictive_equality': pe_results
            }
        
        # ç”Ÿæˆå…¬å¹³æ€§æŠ¥å‘Š
        self.generate_fairness_report(fairness_results, overall)
        
        return {
            'overall_performance': overall,
            'fairness_results': fairness_results
        }
    
    def generate_fairness_report(self, fairness_results, overall_performance):
        """ç”Ÿæˆå…¬å¹³æ€§å®¡è®¡æŠ¥å‘Š"""
        print("\n=========== AIå…¬å¹³æ€§å®¡è®¡æŠ¥å‘Š ===========")
        print("\n1. æ¨¡å‹æ•´ä½“æ€§èƒ½")
        print(f"   å‡†ç¡®ç‡: {overall_performance['accuracy']:.4f}")
        
        print("\n2. å…¬å¹³æ€§æŒ‡æ ‡æ‘˜è¦")
        
        # ä¸ºæ¯ä¸ªå±æ€§æ‰“å°å…³é”®å…¬å¹³æ€§æŒ‡æ ‡
        for attribute, results in fairness_results.items():
            print(f"\n   å±æ€§: {attribute}")
            
            if results['demographic_parity']:
                dp_diff = results['demographic_parity']['max_difference']
                print(f"   - äººå£ç»Ÿè®¡å¹³ä»·å·®å¼‚: {dp_diff:.4f} {'âš ï¸' if dp_diff > 0.1 else 'âœ…'}")
            
            if results['equalized_odds']:
                tpr_diff = results['equalized_odds']['max_tpr_difference']
                fpr_diff = results['equalized_odds']['max_fpr_difference']
                print(f"   - TPRæœ€å¤§å·®å¼‚: {tpr_diff:.4f} {'âš ï¸' if tpr_diff > 0.1 else 'âœ…'}")
                print(f"   - FPRæœ€å¤§å·®å¼‚: {fpr_diff:.4f} {'âš ï¸' if fpr_diff > 0.1 else 'âœ…'}")
        
        print("\n3. å»ºè®®")
        print("   - å¯¹äºå·®å¼‚å¤§äº0.1çš„æŒ‡æ ‡ï¼Œå»ºè®®è¿›ä¸€æ­¥è°ƒæŸ¥å’Œç¼“è§£")
        print("   - è€ƒè™‘ä½¿ç”¨é‡åŠ æƒã€é‡æ–°é‡‡æ ·æˆ–å…¬å¹³æ€§çº¦æŸçš„æ–¹æ³•æ”¹è¿›æ¨¡å‹")
        print("   - å®šæœŸé‡æ–°è¯„ä¼°æ¨¡å‹å…¬å¹³æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–°æ•°æ®ä¸Š")
        print("   - ç¡®ä¿æ¨¡å‹æ–‡æ¡£ä¸­åŒ…å«å…¬å¹³æ€§åˆ†æç»“æœ")
        print("========================================")

# æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨è¿™ä¸ªå…¬å¹³æ€§å®¡è®¡å·¥å…·
def demo_ai_fairness_audit():
    print("=== AIå…¬å¹³æ€§å®¡è®¡å·¥å…·æ¼”ç¤º ===")
    
    # æ³¨æ„ï¼šåœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ‚¨ä¼šåŠ è½½çœŸå®çš„æ•°æ®é›†å’Œè®­ç»ƒå¥½çš„æ¨¡å‹
    # è¿™é‡Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€å•çš„åˆæˆæ•°æ®é›†æ¥æ¼”ç¤ºåŠŸèƒ½
    
    # åˆ›å»ºåˆæˆæ•°æ®é›†
    np.random.seed(42)
    n_samples = 1000
    
    # åˆ›å»ºç‰¹å¾ï¼ˆåŒ…æ‹¬ä¸€äº›å—ä¿æŠ¤å±æ€§ï¼‰
    data = {
        'age': np.random.randint(18, 80, size=n_samples),
        'gender': np.random.choice(['male', 'female'], size=n_samples),
        'race': np.random.choice(['white', 'black', 'asian', 'hispanic'], size=n_samples),
        'income': np.random.normal(50000, 20000, size=n_samples).astype(int),
        'credit_score': np.random.randint(300, 850, size=n_samples)
    }
    
    # åˆ›å»ºä¸€ä¸ªDataFrame
    df = pd.DataFrame(data)
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ ‡ç­¾ï¼ˆè´·æ¬¾æ‰¹å‡†å†³ç­–ï¼‰
    # æœ‰æ„å¼•å…¥ä¸€äº›åè§ï¼Œä½¿æ¼”ç¤ºæ›´æœ‰æ„ä¹‰
    base_approval_rate = 0.6
    gender_bias = np.where(df['gender'] == 'female', 0.1, 0)  # å¯¹å¥³æ€§ç”³è¯·è€…ä¸åˆ©
    race_bias = np.where(df['race'] == 'black', 0.15, 0)     # å¯¹é»‘äººç”³è¯·è€…ä¸åˆ©
    
    # åŸºäºä¿¡ç”¨åˆ†æ•°å’Œåè§åˆ›å»ºæ‰¹å‡†æ¦‚ç‡
    approval_prob = base_approval_rate \
                   + 0.001 * (df['credit_score'] - 500) \
                   - gender_bias \
                   - race_bias
    
    # ç¡®ä¿æ¦‚ç‡åœ¨0-1ä¹‹é—´
    approval_prob = np.clip(approval_prob, 0.1, 0.9)
    
    # åˆ›å»ºæ ‡ç­¾
    df['approved'] = np.random.binomial(1, approval_prob)
    
    # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
    X = df.drop('approved', axis=1)
    y = df['approved']
    
    # ç®€å•çš„æ¨¡å‹æ¨¡æ‹Ÿï¼ˆåœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™ä¼šæ˜¯ä¸€ä¸ªçœŸå®è®­ç»ƒçš„æ¨¡å‹ï¼‰
    class DummyModel:
        def predict(self, X):
            # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿé¢„æµ‹ï¼Œéƒ¨åˆ†åŸºäºä¿¡ç”¨åˆ†æ•°ï¼Œéƒ¨åˆ†éšæœº
            predictions = []
            for _, row in X.iterrows():
                # åŸºäºä¿¡ç”¨åˆ†æ•°çš„åŸºç¡€æ¦‚ç‡
                base_prob = 0.5 + 0.001 * (row['credit_score'] - 500)
                
                # æ·»åŠ ä¸€äº›éšæœºå™ªå£°
                prob = base_prob + np.random.normal(0, 0.1)
                prob = np.clip(prob, 0, 1)
                
                # è¿›è¡Œé¢„æµ‹
                predictions.append(1 if np.random.random() < prob else 0)
            
            return np.array(predictions)
    
    # åˆ›å»ºå¹¶ä½¿ç”¨å®¡è®¡å·¥å…·
    model = DummyModel()
    auditor = AIFairnessAuditor(
        model=model,
        X=X,
        y=y,
        protected_attributes=['gender', 'race']
    )
    
    # è¿è¡Œå®Œæ•´å®¡è®¡
    results = auditor.run_full_audit()
    
    print("\næ¼”ç¤ºå®Œæˆï¼åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ‚¨åº”è¯¥ï¼š")
    print("1. ä½¿ç”¨çœŸå®çš„æ•°æ®é›†å’Œè®­ç»ƒå¥½çš„æ¨¡å‹")
    print("2. æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„å…¬å¹³æ€§æŒ‡æ ‡")
    print("3. è®¾å®šé€‚å½“çš„é˜ˆå€¼æ¥åˆ¤æ–­å…¬å¹³æ€§")
    print("4. é‡‡å–æªæ–½æ¥ç¼“è§£å‘ç°çš„åè§")

if __name__ == "__main__":
    demo_ai_fairness_audit()

# æ³¨æ„ï¼š
# 1. è¿™ä¸ªå·¥å…·æä¾›äº†åŸºæœ¬çš„å…¬å¹³æ€§åˆ†æåŠŸèƒ½ï¼Œå®é™…åº”ç”¨å¯èƒ½éœ€è¦æ›´å¤æ‚çš„æŒ‡æ ‡
# 2. å…¬å¹³æ€§åˆ†æåº”è¯¥ç»“åˆé¢†åŸŸçŸ¥è¯†å’Œå…·ä½“åº”ç”¨åœºæ™¯æ¥è§£é‡Š
# 3. æŠ€æœ¯æªæ–½åº”è¯¥ä¸æ”¿ç­–å’Œæµç¨‹æªæ–½ç›¸ç»“åˆï¼Œå…±åŒç¡®ä¿AIç³»ç»Ÿçš„å…¬å¹³æ€§
# 4. å®šæœŸè¿›è¡Œå…¬å¹³æ€§å®¡è®¡æ˜¯è´Ÿè´£ä»»AIå¼€å‘çš„é‡è¦ç»„æˆéƒ¨åˆ†

### 7.2 è´Ÿè´£ä»»çš„AIå¼€å‘å®è·µ
**[æ ‡è¯†: ETHICS-002]**

è´Ÿè´£ä»»çš„AIå¼€å‘éœ€è¦å°†ä¼¦ç†åŸåˆ™èå…¥æ•´ä¸ªå¼€å‘æµç¨‹ï¼š

- **éšç§è®¾è®¡**ï¼šåœ¨ç³»ç»Ÿè®¾è®¡åˆæœŸå°±è€ƒè™‘éšç§ä¿æŠ¤
- **é€æ˜åº¦æŠ¥å‘Š**ï¼šå…¬å¼€æŠ«éœ²AIç³»ç»Ÿçš„åŠŸèƒ½ã€é™åˆ¶å’Œæ½œåœ¨é£é™©
- **æŒç»­ç›‘æ§**ï¼šå®šæœŸè¯„ä¼°AIç³»ç»Ÿåœ¨å®é™…ä½¿ç”¨ä¸­çš„è¡¨ç°å’Œå½±å“
- **å¤šæ–¹åˆ©ç›Šç›¸å…³è€…å‚ä¸**ï¼šç¡®ä¿ä¸åŒç¾¤ä½“çš„å£°éŸ³è¢«å¬åˆ°
- **æ³•è§„åˆè§„**ï¼šéµå®ˆç›¸å…³çš„æ•°æ®ä¿æŠ¤å’ŒAIæ²»ç†æ³•è§„

## 8. æ€»ç»“ä¸å»ºè®®

### 7.1 æ–°å…´æŠ€æœ¯å­¦ä¹ è·¯å¾„
**[æ ‡è¯†: SUMMARY-001]**

å­¦ä¹ Pythonæ–°å…´æŠ€æœ¯çš„æ¨èè·¯å¾„ï¼š

1. **åŸºç¡€çŸ¥è¯†å·©å›º**ï¼šç¡®ä¿æ‰å®æŒæ¡Pythonæ ¸å¿ƒè¯­æ³•å’Œæ•°æ®ç»“æ„
2. **é¢†åŸŸé€‰æ‹©**ï¼šæ ¹æ®å…´è¶£å’Œåº”ç”¨éœ€æ±‚ï¼Œé€‰æ‹©ç‰¹å®šé¢†åŸŸæ·±å…¥å­¦ä¹ 
3. **å®è·µé¡¹ç›®**ï¼šé€šè¿‡å°å‹é¡¹ç›®ç§¯ç´¯ç»éªŒï¼Œé€æ­¥æ‰©å±•å¤æ‚åº¦
4. **æŒç»­å­¦ä¹ **ï¼šå…³æ³¨æŠ€æœ¯åŠ¨æ€ï¼Œå‚ä¸ç¤¾åŒºè®¨è®ºï¼Œå®šæœŸæ›´æ–°çŸ¥è¯†
5. **è·¨é¢†åŸŸèåˆ**ï¼šæ¢ç´¢ä¸åŒæŠ€æœ¯é¢†åŸŸçš„äº¤å‰åº”ç”¨ï¼Œå¦‚AI+é‡å­è®¡ç®—

### 7.2 ä¼ä¸šåº”ç”¨ç­–ç•¥å»ºè®®
**[æ ‡è¯†: SUMMARY-002]**

ä¼ä¸šé‡‡ç”¨Pythonæ–°å…´æŠ€æœ¯çš„ç­–ç•¥å»ºè®®ï¼š

1. **æŠ€æœ¯è¯„ä¼°**ï¼šå…¨é¢è¯„ä¼°æŠ€æœ¯æˆç†Ÿåº¦å’Œä¸šåŠ¡é€‚ç”¨æ€§
2. **å°è§„æ¨¡è¯•ç‚¹**ï¼šåœ¨éå…³é”®ä¸šåŠ¡åœºæ™¯å…ˆè¡Œå°è¯•
3. **äººæ‰åŸ¹å…»**ï¼šå»ºç«‹å†…éƒ¨åŸ¹è®­æœºåˆ¶ï¼ŒåŸ¹å…»ä¸“ä¸šäººæ‰
4. **ç”Ÿæ€ç³»ç»Ÿå»ºè®¾**ï¼šæ„å»ºæ”¯æŒæ–°æŠ€æœ¯åº”ç”¨çš„å·¥å…·é“¾å’Œæµç¨‹
5. **æŒç»­ä¼˜åŒ–**ï¼šåŸºäºå®é™…åº”ç”¨æ•ˆæœï¼Œä¸æ–­è°ƒæ•´å’Œå®Œå–„æŠ€æœ¯æ–¹æ¡ˆ
```
